# -*- coding: utf-8 -*-
"""Implementation of the ``targeted_seq_cnv_calling`` step

This step allows for the detection of CNV events for germline samples from targeted sequenced
(e.g., exomes or large panels).  The wrapped tools start from the aligned reads (thus off
``ngs_mapping``) and generate CNV calls for germline variants.

The wrapped tools implement different strategies.  Some work "reference free" and just use the
germline BAM files for their input, others need the germline BAM files and additionally a
set of BAM files generated by the same wet-lab and Bioinformatics protocols for their background.

==========
Stability
==========

The XHMM and gCNV from the Genome Analysis Toolkit (GATK) are considered stable.

==========
Step Input
==========

Germline CNV calling for targeted sequencing starts off the aligned reads, i.e., ``ngs_mapping``.

===========
Step Output
===========

For all pedigrees, CNV calling will be performed on the primary DNA NGS libraries of all
members, separately for each configured read mapper and CNV caller.  The name of the primary
DNA NGS library of the index will be used as an identification token in the output file.  For each
read mapper, CNV caller, and pedigree, the following files will be generated:

- ``{mapper}.{cnv_caller}.{lib_name}.vcf.gz``
- ``{mapper}.{cnv_caller}.{lib_name}.vcf.gz.tbi``
- ``{mapper}.{cnv_caller}.{lib_name}.vcf.gz.md5``
- ``{mapper}.{cnv_caller}.{lib_name}.vcf.gz.tbi.md5``

For example, it might look as follows for the example from above:

::

    output/
    +-- bwa.gcnv.P001-N1-DNA1-WES1
    |   `-- out
    |       |-- bwa.gcnv.P001-N1-DNA1-WES1.vcf.gz
    |       |-- bwa.gcnv.P001-N1-DNA1-WES1.vcf.gz.tbi
    |       |-- bwa.gcnv.P001-N1-DNA1-WES1.vcf.gz.md5
    |       `-- bwa.gcnv.P001-N1-DNA1-WES1.vcf.gz.tbi.md5
    [...]

=====================
Default Configuration
=====================

The default configuration is as follows.

.. include:: DEFAULT_CONFIG_targeted_seq_cnv_calling.rst

=====================
Available CNV Callers
=====================

- ``gCNV``
- ``XHMM``

"""

from collections import OrderedDict
from copy import deepcopy
import glob
import json
import os
import re

from biomedsheets.shortcuts import GermlineCaseSheet, is_background, is_not_background
from snakemake.io import expand, glob_wildcards, touch

from snappy_pipeline.base import InvalidConfiguration, UnsupportedActionException
from snappy_pipeline.utils import DictQuery, dictify, listify
from snappy_pipeline.workflows.abstract import BaseStep, BaseStepPart, LinkOutStepPart
from snappy_pipeline.workflows.ngs_mapping import NgsMappingWorkflow

__author__ = "Manuel Holtgrewe <manuel.holtgrewe@bihealth.de>"

#: Extensions of files to create as main payload (VCF)
EXT_VALUES = (".vcf.gz", ".vcf.gz.tbi", ".vcf.gz.md5", ".vcf.gz.tbi.md5")

#: Names of the files to create for the extension
EXT_NAMES = ("vcf", "tbi", "vcf_md5", "tbi_md5")

#: Available WGS CNV callers
TARGETED_SEQ_CNV_CALLERS = ("xhmm", "gcnv")

#: Minimum number of samples using kit - criteria to be analyzed
MIN_KIT_SAMPLES = 10

#: Default configuration for the targeted_seq_cnv_calling step
DEFAULT_CONFIG = r"""
# Default configuration targeted_seq_cnv_calling
step_config:
  targeted_seq_cnv_calling:
    # Path to the ngs_mapping step.
    path_ngs_mapping: ../ngs_mapping

    # List of used tools, by default it only uses XHMM.
    tools:
    - xhmm

    # Tool specific configurations
    xhmm:
      path_target_interval_list: REQUIRED_OR_MAPPING # REQUIRED

      # The following allows to define one or more set of target intervals.
      path_target_interval_list_mapping: []
      # The following will match both the stock IDT library kit and the ones
      # with spike-ins seen from Yale genomics.  The path above would be
      # mapped to the name "default".
      # - name: IDT_xGen_V1_0
      #   pattern: "xGen Exome Research Panel V1\\.0*"
      #   path: "path/to/targets.bed"

    gcnv:
      path_target_interval_list: REQUIRED_OR_MAPPING # REQUIRED

      # Path to gCNV model - will execute analysis in CASE MODE.
      #
      # Example of precomputed model:
      # - library: "Agilent SureSelect Human All Exon V6"  # Library name
      #   contig_ploidy: /path/to/ploidy-model         # Output from `DetermineGermlineContigPloidy`
      #   model_pattern: /path/to/model_*              # Output from `GermlineCNVCaller`
      precomputed_model_paths: []

      # The following allows to define one or more set of target intervals.
      path_target_interval_list_mapping: []
      # The following will match both the stock IDT library kit and the ones
      # with spike-ins seen fromr Yale genomics.  The path above would be
      # mapped to the name "default".
      # - name: IDT_xGen_V1_0
      #   pattern: "xGen Exome Research Panel V1\\.0*"
      #   path: "path/to/targets.bed"

      # Path to BED file with uniquely mappable regions.
      path_uniquely_mapable_bed: REQUIRED
"""


class XhmmStepPart(BaseStepPart):
    """Targeted seq. CNV calling with XHMM"""

    #: Step name
    name = "xhmm"

    #: Class available actions
    actions = (
        "coverage",
        "merge_cov",
        "ref_stats",
        "filter_center",
        "pca",
        "normalize",
        "zscore_center",
        "refilter",
        "discover",
        "genotype",
        "extract_ped",
    )

    def __init__(self, parent):
        super().__init__(parent)
        # Build shortcut from index library name to donor
        self.index_ngs_library_to_donor = OrderedDict()
        for sheet in self.parent.shortcut_sheets:
            self.index_ngs_library_to_donor.update(sheet.index_ngs_library_to_donor)
        # Build shortcut from index library name to pedigree
        self.donor_ngs_library_to_pedigree = OrderedDict()
        for sheet in self.parent.shortcut_sheets:
            self.donor_ngs_library_to_pedigree.update(sheet.donor_ngs_library_to_pedigree)
        # Build shortcut from index library name to pedigree
        self.index_ngs_library_to_pedigree = OrderedDict()
        for sheet in self.parent.shortcut_sheets:
            self.index_ngs_library_to_pedigree.update(sheet.index_ngs_library_to_pedigree)
        # Take shortcut from library to library kit.
        self.ngs_library_to_kit = self._build_ngs_library_to_kit()

    def get_params(self, action):
        """
        :param action: Action (i.e., step) in the workflow. Currently only available for 'coverage'.
        :type action: str

        :return: Returns input function for XHMM rule based on inputted action.

        :raises UnsupportedActionException: if action not 'coverage'.
        """
        # Validate inputted action
        if action != "coverage":
            error_message = "Action '{action}' is not supported. Valid options: 'coverage'.".format(
                action=action
            )
            raise UnsupportedActionException(error_message)

        def get_params(wildcards):
            return {"library_kit": self.ngs_library_to_kit[wildcards.library_name]}

        return get_params

    @dictify
    def _build_ngs_library_to_kit(self):
        xhmm_config = DictQuery(self.w_config).get("step_config/targeted_seq_cnv_calling/xhmm")
        if not xhmm_config["path_target_interval_list_mapping"]:
            # No mapping given, we will use the "default" one for all.
            for donor in self.parent.all_donors():
                if donor.dna_ngs_library:
                    yield donor.dna_ngs_library.name, "default"

        # Build mapping.
        regexes = {
            item["pattern"]: item["name"]
            for item in xhmm_config["path_target_interval_list_mapping"]
        }
        result = {}
        for donor in self.parent.all_donors():
            if donor.dna_ngs_library and donor.dna_ngs_library.extra_infos.get("libraryKit"):
                library_kit = donor.dna_ngs_library.extra_infos.get("libraryKit")
                for pattern, name in regexes.items():
                    if re.match(pattern, library_kit):
                        yield donor.dna_ngs_library.name, name
        return result

    def get_input_files(self, action):
        """Return input function for XHMM rule

        :param action: Action (i.e., step) in the workflow, examples: 'coverage',
        'filter_center', 'extract_ped'.
        :type action: str

        :return: Returns input function for gCNV rule based on inputted action.

        :raises UnsupportedActionException: if action not in class defined list of valid actions.
        """
        # Validate inputted action
        valid_actions = [a for a in self.actions if a != "ref_stats"]
        if action not in valid_actions:
            valid_actions_str = ", ".join(valid_actions)
            error_message = "Action '{action}' is not supported. Valid options: {options}".format(
                action=action, options=valid_actions_str
            )
            raise UnsupportedActionException(error_message)
        # Return requested function
        return getattr(self, "_get_input_files_{}".format(action))

    @dictify
    def _get_input_files_coverage(self, wildcards):
        ngs_mapping = self.parent.sub_workflows["ngs_mapping"]
        # Yield input BAM and BAI file
        bam_tpl = "output/{mapper}.{library_name}/out/{mapper}.{library_name}{ext}"
        for key, ext in {"bam": ".bam", "bai": ".bam.bai"}.items():
            yield key, ngs_mapping(bam_tpl.format(ext=ext, **wildcards))

    def _get_input_files_merge_cov(self, wildcards):
        name_pattern = "{mapper}.xhmm_coverage.{lib}"
        summaries = [
            "work/{name_pattern}/out/{name_pattern}.DATA.sample_interval_summary".format(
                name_pattern=name_pattern
            ).format(lib=lib, **wildcards)
            for lib in sorted(self.index_ngs_library_to_donor)
            if self.ngs_library_to_kit.get(lib) == wildcards.library_kit
        ]
        return summaries

    @dictify
    def _get_input_files_filter_center(self, wildcards):
        yield "merge_cov", self._get_output_files_merge_cov()[0].format(**wildcards)
        yield "extreme_gc", self._get_output_files_ref_stats()["extreme_gc_targets"].format(
            **wildcards
        )

    def _get_input_files_pca(self, wildcards):
        return [self._get_output_files_filter_center()["centered"].format(**wildcards)]

    @dictify
    def _get_input_files_normalize(self, wildcards):
        yield "centered", self._get_output_files_filter_center()["centered"].format(**wildcards)
        yield "pca", self._get_output_files_pca()["pc"].format(**wildcards)

    @staticmethod
    def _get_input_files_zscore_center(wildcards):
        name_pattern = "{mapper}.xhmm_normalize.{library_kit}".format(**wildcards)
        return ["work/{name_pattern}/out/{name_pattern}".format(name_pattern=name_pattern)]

    @staticmethod
    @dictify
    def _get_input_files_refilter(wildcards):
        name_pattern = "{mapper}.xhmm_merge_cov.{library_kit}".format(**wildcards)
        yield "original", "work/{name_pattern}/out/{name_pattern}.RD.txt".format(
            name_pattern=name_pattern
        )
        for infix in ("filter_center", "zscore_center"):
            for kvs in (
                ("filtered_samples", ".filtered_samples.txt"),
                ("filtered_targets", ".filtered_targets.txt"),
            ):
                name_pattern = "{mapper}.xhmm_{infix}.{library_kit}".format(
                    infix=infix, **wildcards
                )
                key = "{}_{}".format(kvs[0], infix)
                yield key, "work/{name_pattern}/out/{name_pattern}{suffix}".format(
                    name_pattern=name_pattern, suffix=kvs[1]
                )

    @staticmethod
    @dictify
    def _get_input_files_discover(wildcards):
        name_pattern = "{mapper}.xhmm_zscore_center.{library_kit}".format(**wildcards)
        yield "center_zscore", "work/{name_pattern}/out/{name_pattern}".format(
            name_pattern=name_pattern
        )
        name_pattern = "{mapper}.xhmm_refilter.{library_kit}".format(**wildcards)
        yield "refilter_original", "work/{name_pattern}/out/{name_pattern}.RD.txt".format(
            name_pattern=name_pattern
        )

    @staticmethod
    @dictify
    def _get_input_files_genotype(wildcards):
        name_pattern = "{mapper}.xhmm_zscore_center.{library_kit}".format(**wildcards)
        yield "center_zscore", "work/{name_pattern}/out/{name_pattern}".format(
            name_pattern=name_pattern
        )
        name_pattern = "{mapper}.xhmm_refilter.{library_kit}".format(**wildcards)
        yield "refilter_original", "work/{name_pattern}/out/{name_pattern}.RD.txt".format(
            name_pattern=name_pattern
        )
        name_pattern = "{mapper}.xhmm_discover.{library_kit}".format(**wildcards)
        yield "discover_xcnv", "work/{name_pattern}/out/{name_pattern}.xcnv".format(
            name_pattern=name_pattern
        )

    @dictify
    def _get_input_files_extract_ped(self, wildcards):
        library_kit = self.ngs_library_to_kit[wildcards.library_name]
        name_pattern = "bwa.xhmm_filter_center.{library_kit}".format(library_kit=library_kit)
        yield (
            "filtered_samples",
            "work/{name_pattern}/out/{name_pattern}.filtered_samples.txt".format(
                name_pattern=name_pattern
            ),
        )
        name_pattern = "{mapper}.xhmm_genotype.{library_kit}".format(
            library_kit=library_kit, **wildcards
        )
        for key, ext in (("vcf", ".vcf.gz"), ("tbi", ".vcf.gz.tbi")):
            yield key, "work/{name_pattern}/out/{name_pattern}{ext}".format(
                name_pattern=name_pattern, ext=ext
            )

    def get_ped_members(self, wildcards):
        pedigree = self.index_ngs_library_to_pedigree[wildcards.library_name]
        return " ".join(
            donor.dna_ngs_library.name for donor in pedigree.donors if donor.dna_ngs_library
        )

    def get_output_files(self, action):
        """Return output files that XHMM creates for the given action."""
        assert action in self.actions
        return getattr(self, "_get_output_files_{}".format(action))()

    @staticmethod
    @dictify
    def _get_output_files_coverage():
        exts = (
            "sample_interval_statistics",
            "sample_interval_summary",
            "sample_statistics",
            "sample_summary",
        )
        for ext in exts:
            name_pattern = "{mapper}.xhmm_coverage.{library_name}"
            yield ext, "work/{name_pattern}/out/{name_pattern}.DATA.{ext}".format(
                name_pattern=name_pattern, ext=ext
            )

    @staticmethod
    def _get_output_files_merge_cov():
        name_pattern = "{mapper}.xhmm_merge_cov.{library_kit}"
        return ["work/{name_pattern}/out/{name_pattern}.RD.txt".format(name_pattern=name_pattern)]

    @staticmethod
    @dictify
    def _get_output_files_ref_stats():
        name_pattern = "{mapper}.xhmm_ref_stats.{library_kit}"
        for infix in ("extreme_gc_targets",):
            yield infix, "work/{name_pattern}/out/{name_pattern}.{infix}.txt".format(
                name_pattern=name_pattern, infix=infix
            )

    @staticmethod
    @dictify
    def _get_output_files_filter_center():
        name_pattern = "{mapper}.xhmm_filter_center.{library_kit}"
        for infix in ("centered", "filtered_targets", "filtered_samples"):
            yield infix, "work/{name_pattern}/out/{name_pattern}.{infix}.txt".format(
                name_pattern=name_pattern, infix=infix
            )

    @staticmethod
    @dictify
    def _get_output_files_pca():
        name_pattern = "{mapper}.xhmm_pca.{library_kit}"
        kvs = (("pc_loading", ".PC_LOADINGS.txt"), ("pc_sd", ".PC_SD.txt"), ("pc", ".PC.txt"))
        for key, suffix in kvs:
            yield key, "work/{name_pattern}/out/{name_pattern}{suffix}".format(
                name_pattern=name_pattern, suffix=suffix
            )

    @staticmethod
    @dictify
    def _get_output_files_normalize():
        name_pattern = "{mapper}.xhmm_normalize.{library_kit}"
        kvs = (("normalized", ""), ("num_removed", ".num_removed_PC.txt"))
        for key, suffix in kvs:
            yield key, "work/{name_pattern}/out/{name_pattern}{suffix}".format(
                name_pattern=name_pattern, suffix=suffix
            )

    @staticmethod
    @dictify
    def _get_output_files_zscore_center():
        name_pattern = "{mapper}.xhmm_zscore_center.{library_kit}"
        kvs = (
            ("zscore_center", ""),
            ("filtered_samples", ".filtered_samples.txt"),
            ("filtered_targets", ".filtered_targets.txt"),
        )
        for key, suffix in kvs:
            yield key, "work/{name_pattern}/out/{name_pattern}{suffix}".format(
                name_pattern=name_pattern, suffix=suffix
            )

    @staticmethod
    def _get_output_files_refilter():
        name_pattern = "{mapper}.xhmm_refilter.{library_kit}"
        return ["work/{name_pattern}/out/{name_pattern}.RD.txt".format(name_pattern=name_pattern)]

    @staticmethod
    @dictify
    def _get_output_files_discover():
        name_pattern = "{mapper}.xhmm_discover.{library_kit}"
        kvs = (("xcnv", ".xcnv"), ("aux_xcnv", ".aux_xcnv"))
        for key, suffix in kvs:
            yield key, "work/{name_pattern}/out/{name_pattern}{suffix}".format(
                name_pattern=name_pattern, suffix=suffix
            )

    @staticmethod
    @dictify
    def _get_output_files_genotype():
        name_pattern = "{mapper}.xhmm_genotype.{library_kit}"
        kvs = (
            ("vcf", ".vcf.gz"),
            ("vcf_md5", ".vcf.gz.md5"),
            ("tbi", ".vcf.gz.tbi"),
            ("tbi_md5", ".vcf.gz.tbi.md5"),
        )
        for key, suffix in kvs:
            yield key, "work/{name_pattern}/out/{name_pattern}{suffix}".format(
                name_pattern=name_pattern, suffix=suffix
            )

    @staticmethod
    @dictify
    def _get_output_files_extract_ped():
        name_pattern = "{mapper}.xhmm.{library_name}"
        kvs = (
            ("vcf", ".vcf.gz"),
            ("vcf_md5", ".vcf.gz.md5"),
            ("tbi", ".vcf.gz.tbi"),
            ("tbi_md5", ".vcf.gz.tbi.md5"),
        )
        for key, suffix in kvs:
            yield key, "work/{name_pattern}/out/{name_pattern}{suffix}".format(
                name_pattern=name_pattern, suffix=suffix
            )

    @staticmethod
    def get_log_file(action):
        """Return path to log file"""
        if action == "coverage":
            return (
                "work/{{mapper}}.xhmm_{action}.{{library_name}}/log/"
                "snakemake.targeted_seq_cnv_calling.log"
            ).format(action=action)
        elif action == "extract_ped":
            return "work/{mapper}.xhmm.{library_name}/log/" "snakemake.targeted_seq_cnv_calling.log"
        else:
            return (
                "work/{{mapper}}.xhmm_{action}.{{library_kit}}/log/"
                "snakemake.targeted_seq_cnv_calling.log"
            ).format(action=action)

    def update_cluster_config(self, cluster_config):
        """Update cluster configuration for XHMM CNV calling"""
        for action in self.actions:
            if action == "merge_cov":
                cluster_config["targeted_seq_cnv_calling_xhmm_{}".format(action)] = {
                    "mem": 12 * 1024,
                    "time": "24:00",
                    "ntasks": 1,
                }
            else:
                cluster_config["targeted_seq_cnv_calling_xhmm_{}".format(action)] = {
                    "mem": 12 * 1024,
                    "time": "08:00",
                    "ntasks": 1,
                }


class GcnvStepPart(BaseStepPart):
    """Targeted seq. CNV calling with GATK4 gCNV"""

    #: Step name
    name = "gcnv"

    #: Class available actions
    actions = (
        "preprocess_intervals",
        "annotate_gc",
        "filter_intervals",
        "scatter_intervals",
        "coverage",
        "contig_ploidy",
        "contig_ploidy_case_mode",
        "call_cnvs_cohort_mode",
        "call_cnvs_case_mode",
        "post_germline_calls",
        "post_germline_calls_cohort_mode",
        "post_germline_calls_case_mode",
        "merge_cohort_vcfs",
        "extract_ped",
    )

    def __init__(self, parent):
        """Constructor."""
        super().__init__(parent)
        # Build shortcut from index library name to donor
        self.index_ngs_library_to_donor = OrderedDict()
        for sheet in self.parent.shortcut_sheets:
            self.index_ngs_library_to_donor.update(sheet.index_ngs_library_to_donor)
        # Build shortcut from index library name to pedigree
        self.donor_ngs_library_to_pedigree = OrderedDict()
        for sheet in self.parent.shortcut_sheets:
            self.donor_ngs_library_to_pedigree.update(sheet.donor_ngs_library_to_pedigree)
        # Build shortcut from index library name to pedigree
        self.index_ngs_library_to_pedigree = OrderedDict()
        for sheet in self.parent.shortcut_sheets:
            self.index_ngs_library_to_pedigree.update(sheet.index_ngs_library_to_pedigree)
        # Take shortcut from library to library kit.
        self.ngs_library_to_kit = self._build_ngs_library_to_kit()
        # Get type of run: 'cohort_mode' or 'case_mode'.
        self.analysis_type = self.validate_request()

    def validate_request(self):
        """Validate request.

        Checks if the request can be performed given the provided information and parameters.
        Three scenarios are evaluated:
        1) If no model was provided in the config (``precomputed_model_paths``), no check is
        required. Analysis will run in COHORT MODE and build a new model with all foreground
        samples available for each library kit.

        2) If a path to a model was provided in the config (``precomputed_model_paths``), it will
        check that the path were provided for each available library kit and that the paths
        contain the necessary files for analysis, namely: ...
        Analysis will run in CASE MODE for each sample.

        :return: Returns type of analysis: 'cohort_mode' or 'case_mode'.

        :raises InvalidConfiguration: if information provided in configuration isn't enough to run
        the analysis.
        """
        # Get precomputed models from configurations
        path_to_models = self.config["gcnv"]["precomputed_model_paths"]

        # Case 1: no model provided -> analysis in COHORT MODE
        if len(path_to_models) == 0:
            return "cohort_mode"
        else:
            # Case 2: path to model provided -> analysis in CASE MODE using precomputed model

            # Validate configuration - check if only expected keys are present
            self.validate_precomputed_model_paths_config(config=path_to_models)

            # Check model directories content
            for model in path_to_models:
                # Validate ploidy-model
                ploidy_path = model.get("contig_ploidy")
                if not self.validate_ploidy_model_directory(path=ploidy_path):
                    msg_tpl = (
                        "Provided path either not a directory or "
                        "does not contain all required contig-ploidy model files: {0}"
                    )
                    raise InvalidConfiguration(msg_tpl.format(str(model)))
                # Find all model directories
                model_pattern = model.get("model_pattern")
                path_list = self.get_model_dir_list(pattern=model_pattern)
                if len(path_list) == 0:
                    msg_tpl = "Could not find any directory path with the provided pattern: '{0}' "
                    raise InvalidConfiguration(msg_tpl.format(model_pattern))
                # Validate directory
                for path in path_list:
                    if not self.validate_call_model_directory(path=path):
                        msg_tpl = (
                            "Provided path either not a directory or "
                            "does not contain all required call model files: {0}"
                        )
                        raise InvalidConfiguration(msg_tpl.format(str(model)))
            # All checked out - return
            return "case_mode"

    def validate_precomputed_model_paths_config(self, config):
        """Validate precomputed model config.

        Evaluates if provided configuration has the following format:

        precomputed_model_paths:
          - library: "Agilent SureSelect Human All Exon V6"
            contig_ploidy": /path/to/ploidy-model
            model_pattern: /path/to/model_*

        :param config: List of precomputed model configuration dictionary.
        :type config: list

        :raises InvalidConfiguration: if configuration not as expected for
        ``precomputed_model_paths`` list.
        """
        # Initialise variables
        expected_keys = ("library", "model_pattern", "contig_ploidy")
        expected_format = (
            '{\n    "library": "Agilent SureSelect Human All Exon V6"\n'
            '    "contig_ploidy": /path/to/ploidy-model\n'
            '    "model_pattern": "/path/to/model_*"\n}'
        )
        # Test
        for model in config:
            # Test keys
            n_keys_pass = len(model) == 3
            keys_pass = all(key in expected_keys for key in model)
            # Test values
            values_pass = all(isinstance(value, str) for value in model.values())
            # Validate
            if not (n_keys_pass and keys_pass and values_pass):
                msg_tpl = (
                    "Provided configuration not as expected...\n"
                    "Expected:\n{e_}\nObserved:\n{o_}\n"
                )
                pretty_model = self._pretty_print_config(config=model)
                raise InvalidConfiguration(msg_tpl.format(e_=expected_format, o_=pretty_model))

    @staticmethod
    def _pretty_print_config(config):
        """Pretty format configuration.

        :param config: Configuration dictionary to be formatted.
        :type config: OrderedDict

        :return: Configuration as a nicely formatted string.
        """
        return str(json.dumps(config, sort_keys=False, indent=4))

    @staticmethod
    def validate_ploidy_model_directory(path):
        """Validate gCNV ploidy-model directory.

        :param path: Path to gCNV ploidy-model directory.
        :return: Returns True if path is a directory and contains all required files; otherwise,
        False.
        """
        # Model required files
        model_files = [
            "contig_ploidy_prior.tsv",
            "gcnvkernel_version.json",
            "interval_list.tsv",
            "mu_mean_bias_j_lowerbound__.tsv",
            "mu_psi_j_log__.tsv",
            "ploidy_config.json",
            "std_mean_bias_j_lowerbound__.tsv",
            "std_psi_j_log__.tsv",
        ]
        # Check if path is a directory
        if not os.path.isdir(path):
            return False
        # Check if directory contains required model files
        if not all(os.path.isfile(os.path.join(path, m_file)) for m_file in model_files):
            return False
        return True

    @staticmethod
    def validate_call_model_directory(path):
        """Validate gCNV call-model directory.

        :param path: Path to gCNV call-model directory. Files created using COHORT mode.
        :return: Returns True if path is a directory and contains all required files; otherwise,
        False.
        """
        # Model required files
        model_files = [
            "calling_config.json",
            "gcnvkernel_version.json",
            "log_q_tau_tk.tsv",
            "mu_ard_u_log__.tsv",
            "mu_psi_t_log__.tsv",
            "std_ard_u_log__.tsv",
            "std_psi_t_log__.tsv",
            "denoising_config.json",
            "interval_list.tsv",
            "mu_W_tu.tsv",
            "mu_log_mean_bias_t.tsv",
            "std_W_tu.tsv",
            "std_log_mean_bias_t.tsv",
        ]
        # Check if path is a directory
        if not os.path.isdir(path):
            return False
        # Check if directory contains required model files
        if not all(os.path.isfile(os.path.join(path, m_file)) for m_file in model_files):
            return False
        return True

    @staticmethod
    def get_model_dir_list(pattern):
        """Get model directories.

        :param pattern: Pattern of model directory paths. Expects models to be based on scattered
        step: ``gatk IntervalListTools``.
        :type pattern: str

        :return: Returns list with all directories that match the inputted pattern.
        """
        return [path_ for path_ in glob.glob(pattern) if os.path.isdir(path_)]

    def get_params(self, action):
        """
        :param action: Action (i.e., step) in the workflow. Currently available for:
        'ploidy-model', 'model', and 'postgermline_models'.
        :type action: str

        :return: Returns input function for gCNV rule based on inputted action.

        :raises UnsupportedActionException: if invalid action.
        """
        # Actions with parameters
        valid_actions = ("model", "ploidy_model", "postgermline_models")
        # Validate inputted action
        if action not in valid_actions:
            error_message = "Action '{action}' is not supported. Valid options: {options}.".format(
                action=action, options=", ".join(valid_actions)
            )
            raise UnsupportedActionException(error_message)

        # Return requested function
        return getattr(self, "_get_params_{}".format(action))

    def _get_params_ploidy_model(self, wildcards):
        """Get ploidy-model parameters.

        :param wildcards: Snakemake wildcards associated with rule, namely: 'library_kit'
        (e.g., 'Agilent_SureSelect_Human_All_Exon_V6').
        :type wildcards: snakemake.io.Wildcards

        :return: Returns ploidy-model parameters dictionary if analysis type is 'case_mode';
        otherwise, returns empty dictionary. Step: Calling autosomal and allosomal contig ploidy
        with `DetermineGermlineContigPloidy`.
        """
        analysis_type = self.get_analysis_type()
        if analysis_type == "case_mode":
            path = "__no_ploidy_model_for_library_in_config__"
            for model in self.config["gcnv"]["precomputed_model_paths"]:
                # Adjust library kit name from config to wildcard
                library_to_wildcard = model.get("library").strip().replace(" ", "_")
                if library_to_wildcard == wildcards.library_kit:
                    path = model.get("contig_ploidy")
            return {"model": path}
        else:
            return {}

    def _get_params_model(self, wildcards):
        """Get model parameters.

        :param wildcards: Snakemake wildcards associated with rule, namely: 'library_kit'
        (e.g., 'Agilent_SureSelect_Human_All_Exon_V6').
        :type wildcards: snakemake.io.Wildcards

        :return: Returns model parameters dictionary if analysis type is 'case_mode';
        otherwise, returns empty dictionary. Step: Calling copy number variants with
        `GermlineCNVCaller`.
        """
        analysis_type = self.get_analysis_type()
        if analysis_type == "case_mode":
            path = "__no_model_for_library_in_config__"
            for model in self.config["gcnv"]["precomputed_model_paths"]:
                # Adjust library kit name from config to wildcard
                library_to_wildcard = model.get("library").strip().replace(" ", "_")
                if library_to_wildcard == wildcards.library_kit:
                    pattern = model.get("model_pattern")
                    model_dir_dict = self._get_model_dir_to_dict(pattern)
                    path = model_dir_dict.get(wildcards.shard)
            return {"model": path}
        else:
            return {}

    def _get_params_postgermline_models(self, wildcards):
        """Get post germline model parameters.

        :param wildcards: Snakemake wildcards associated with rule, namely: 'library_name'
        (e.g., 'P001-N1-DNA1-WGS1').
        :type wildcards: snakemake.io.Wildcards

        :return: Returns model parameters dictionary if analysis type is 'case_mode';
        otherwise, returns empty dictionary. Step: consolidating the scattered
        `GermlineCNVCaller` results, performs segmentation and calls copy number states with
        `PostprocessGermlineCNVCalls `.
        """
        analysis_type = self.get_analysis_type()
        if analysis_type == "case_mode":
            paths = ["__no_model_available_for_library__"]
            for model in self.config["gcnv"]["precomputed_model_paths"]:
                # Adjust library kit name from config to wildcard
                library_to_wildcard = model.get("library").strip().replace(" ", "_")
                # Get library kit associated with library name
                library_kit = self.ngs_library_to_kit[wildcards.library_name]
                if library_to_wildcard == library_kit:
                    pattern = model.get("model_pattern")
                    model_dir_dict = self._get_model_dir_to_dict(pattern)
                    paths = list(model_dir_dict.values())
            return {"model": paths}
        else:
            return {}

    def _get_model_dir_to_dict(self, pattern):
        """Get model directories dictionary.

        :param pattern: Pattern of model directory paths. Expects models to be based on scattered
        step: ``gatk IntervalListTools``.
        :type pattern: str

        :return: Returns dictionary with model directories information.
        Key: shard (e.g., '01_of_42'); Value: directory path (e.g., '/path/to/model_01_of_42').
        """
        # Initialise variables
        out_dict = {}
        default_key = "001"
        re_pattern = pattern.replace("*", "(.*)")
        # Get all model directories
        path_list = self.get_model_dir_list(pattern)
        # Populate dictionary
        # Assumption: if no group, a single path was provided instead of a pattern.
        for path in path_list:
            try:
                key = re.search(re_pattern, path).group(1)
                out_dict[key] = path
            except IndexError:
                out_dict[default_key] = path
        return out_dict

    @dictify
    def _build_ngs_library_to_kit(self):
        gcnv_config = DictQuery(self.w_config).get("step_config/targeted_seq_cnv_calling/gcnv")
        if not gcnv_config["path_target_interval_list_mapping"]:
            # No mapping given, we will use the "default" one for all.
            for donor in self.parent.all_donors():
                if donor.dna_ngs_library:
                    yield donor.dna_ngs_library.name, "default"

        # Build mapping
        regexes = {
            item["pattern"]: item["name"]
            for item in gcnv_config["path_target_interval_list_mapping"]
        }
        result = {}
        for donor in self.parent.all_donors():
            if donor.dna_ngs_library and donor.dna_ngs_library.extra_infos.get("libraryKit"):
                library_kit = donor.dna_ngs_library.extra_infos.get("libraryKit")
                for pattern, name in regexes.items():
                    if re.match(pattern, library_kit):
                        yield donor.dna_ngs_library.name, name
        return result

    def get_input_files(self, action):
        """Return input function for gCNV rule

        :param action: Action (i.e., step) in the workflow, examples: 'filter_intervals',
        'coverage', 'extract_ped'.
        :type action: str

        :return: Returns input function for gCNV rule based on inputted action.

        :raises UnsupportedActionException: if action not in class defined list of valid actions.
        """
        # Validate inputted action
        if action not in self.actions:
            valid_actions_str = ", ".join(self.actions)
            error_message = "Action '{action}' is not supported. Valid options: {options}".format(
                action=action, options=valid_actions_str
            )
            raise UnsupportedActionException(error_message)
        # Return requested function
        return getattr(self, "_get_input_files_{}".format(action))

    @staticmethod
    def _get_input_files_preprocess_intervals(wildcards):
        _ = wildcards
        return {}

    @staticmethod
    @dictify
    def _get_input_files_annotate_gc(wildcards):
        name_pattern = "gcnv_preprocess_intervals.{wildcards.library_kit}".format(
            wildcards=wildcards
        )
        ext = "interval_list"
        yield ext, "work/{name_pattern}/out/{name_pattern}.{ext}".format(
            name_pattern=name_pattern, ext=ext
        )

    @dictify
    def _get_input_files_filter_intervals(self, wildcards):
        yield from self._get_input_files_annotate_gc(wildcards).items()
        name_pattern = "gcnv_annotate_gc.{wildcards.library_kit}".format(wildcards=wildcards)
        ext = "tsv"
        yield ext, "work/{name_pattern}/out/{name_pattern}.{ext}".format(
            name_pattern=name_pattern, ext=ext
        )
        key = "covs"
        covs = []
        for lib in sorted(self.index_ngs_library_to_donor):
            if self.ngs_library_to_kit.get(lib) == wildcards.library_kit:
                name_pattern = "{mapper}.gcnv_coverage.{library_name}".format(
                    mapper=wildcards.mapper, library_name=lib
                )
                covs.append(
                    "work/{name_pattern}/out/{name_pattern}.{ext}".format(
                        name_pattern=name_pattern, ext="tsv"
                    )
                )
        yield key, covs

    @staticmethod
    @dictify
    def _get_input_files_scatter_intervals(wildcards):
        ext = "interval_list"
        name_pattern = "{mapper}.gcnv_filter_intervals.{library_kit}".format(**wildcards)
        yield ext, "work/{name_pattern}/out/{name_pattern}.{ext}".format(
            name_pattern=name_pattern, ext=ext
        )

    @dictify
    def _get_input_files_contig_ploidy(self, wildcards):
        """Yield input files for ``contig_ploidy`` rule in COHORT MODE.

        :param wildcards: Snakemake wildcards associated with rule, namely: 'mapper' (e.g., 'bwa')
        and 'library_kit' (e.g., 'Agilent_SureSelect_Human_All_Exon_V6').
        :type wildcards: snakemake.io.Wildcards
        """
        ext = "interval_list"
        name_pattern = "{mapper}.gcnv_filter_intervals.{library_kit}"
        yield ext, "work/{name_pattern}/out/{name_pattern}.{ext}".format(
            name_pattern=name_pattern, ext=ext
        )
        ext = "tsv"
        tsvs = []
        for lib in sorted(self.index_ngs_library_to_donor):
            if self.ngs_library_to_kit.get(lib) == wildcards.library_kit:
                name_pattern = "{mapper}.gcnv_coverage.{library_name}".format(
                    mapper=wildcards.mapper, library_name=lib
                )
                tsvs.append(
                    "work/{name_pattern}/out/{name_pattern}.{ext}".format(
                        name_pattern=name_pattern, ext=ext
                    )
                )
        yield ext, tsvs

    @dictify
    def _get_input_files_contig_ploidy_case_mode(self, wildcards):
        """Yield input files for ``contig_ploidy`` rule  in CASE MODE using precomputed model.

        :param wildcards: Snakemake wildcards associated with rule, namely: 'mapper' (e.g., 'bwa')
        and 'library_kit' (e.g., 'Agilent_SureSelect_Human_All_Exon_V6').
        :type wildcards: snakemake.io.Wildcards
        """
        ext = "tsv"
        tsvs = []
        for lib in sorted(self.index_ngs_library_to_donor):
            if self.ngs_library_to_kit.get(lib) == wildcards.library_kit:
                name_pattern = "{mapper}.gcnv_coverage.{library_name}".format(
                    mapper=wildcards.mapper, library_name=lib
                )
                tsvs.append(
                    "work/{name_pattern}/out/{name_pattern}.{ext}".format(
                        name_pattern=name_pattern, ext=ext
                    )
                )
        yield ext, tsvs

    @dictify
    def _get_input_files_coverage(self, wildcards):
        """Yield input files for ``coverage`` in both COHORT and CASE modes.

        :param wildcards: Snakemake wildcards associated with rule, namely: 'mapper' (e.g., 'bwa')
        and 'library_name' (e.g., 'P001-N1-DNA1-WGS1').
        :type wildcards: snakemake.io.Wildcards
        """
        # Yield .interval list file.
        ext = "interval_list"
        library_kit = self.ngs_library_to_kit[wildcards.library_name]
        name_pattern = "gcnv_preprocess_intervals.{library_kit}".format(library_kit=library_kit)
        yield ext, "work/{name_pattern}/out/{name_pattern}.{ext}".format(
            name_pattern=name_pattern, ext=ext
        )
        # Yield input BAM and BAI files
        ngs_mapping = self.parent.sub_workflows["ngs_mapping"]
        bam_tpl = "output/{mapper}.{library_name}/out/{mapper}.{library_name}{ext}"
        for key, ext in {"bam": ".bam", "bai": ".bam.bai"}.items():
            yield key, ngs_mapping(bam_tpl.format(ext=ext, **wildcards))

    @dictify
    def _get_input_files_call_cnvs_cohort_mode(self, wildcards):
        """Yield input files for ``call_cnvs`` in COHORT mode.

        :param wildcards: Snakemake wildcards associated with rule, namely: 'mapper' (e.g., 'bwa')
        and 'library_kit' (e.g., 'Agilent_SureSelect_Human_All_Exon_V6').
        :type wildcards: snakemake.io.Wildcards
        """
        path_pattern = (
            "work/{name_pattern}/out/{name_pattern}/temp_{{shard}}/scattered.interval_list"
        )
        name_pattern = "{mapper}.gcnv_scatter_intervals.{library_kit}"
        yield "interval_list_shard", path_pattern.format(name_pattern=name_pattern)
        ext = "tsv"
        tsvs = []
        for lib in sorted(self.index_ngs_library_to_donor):
            if self.ngs_library_to_kit.get(lib) == wildcards.library_kit:
                path_pattern = "{mapper}.gcnv_coverage.{library_name}".format(
                    mapper=wildcards.mapper, library_name=lib
                )
                tsvs.append(
                    "work/{name_pattern}/out/{name_pattern}.{ext}".format(
                        name_pattern=path_pattern, ext=ext
                    )
                )
        yield ext, tsvs
        ext = "ploidy"
        path_pattern = "{mapper}.gcnv_contig_ploidy.{library_kit}".format(**wildcards)
        yield ext, "work/{name_pattern}/out/{name_pattern}/.done".format(name_pattern=path_pattern)
        key = "intervals"
        path_pattern = "gcnv_annotate_gc.{library_kit}"
        yield key, "work/{name_pattern}/out/{name_pattern}.{ext}".format(
            name_pattern=path_pattern, ext="tsv"
        )

    @dictify
    def _get_input_files_call_cnvs_case_mode(self, wildcards):
        """Yield input files for ``call_cnvs`` in CASE mode.

        :param wildcards: Snakemake wildcards associated with rule, namely: 'mapper' (e.g., 'bwa')
        and 'library_kit' (e.g., 'Agilent_SureSelect_Human_All_Exon_V6').
        :type wildcards: snakemake.io.Wildcards
        """
        # Initialise variables
        tsv_ext = "tsv"
        tsv_path_pattern = "{mapper}.gcnv_coverage.{library_name}"
        ploidy_ext = "ploidy"
        ploidy_path_pattern = "{mapper}.gcnv_contig_ploidy_case_mode.{library_kit}"

        # Yield coverage tsv files for all library associated with kit
        coverage_files = []
        for lib in sorted(self.index_ngs_library_to_donor):
            if self.ngs_library_to_kit.get(lib) == wildcards.library_kit:
                path_pattern = tsv_path_pattern.format(mapper=wildcards.mapper, library_name=lib)
                coverage_files.append(
                    "work/{name_pattern}/out/{name_pattern}.{ext}".format(
                        name_pattern=path_pattern, ext=tsv_ext
                    )
                )
        yield tsv_ext, coverage_files

        # Yield ploidy files
        path_pattern = ploidy_path_pattern.format(**wildcards)
        yield ploidy_ext, "work/{name_pattern}/out/{name_pattern}/.done".format(
            name_pattern=path_pattern
        )

    @dictify
    def _get_input_files_post_germline_calls_cohort_mode(self, wildcards, checkpoints):
        checkpoint = checkpoints.targeted_seq_cnv_calling_gcnv_scatter_intervals
        library_kit = self.ngs_library_to_kit.get(wildcards.library_name)
        scatter_out = checkpoint.get(library_kit=library_kit, **wildcards).output[0]
        shards = list(
            map(
                os.path.basename,
                glob_wildcards(os.path.join(scatter_out, "temp_{shard}/{file}")).shard,
            )
        )
        name_pattern = "{mapper}.gcnv_call_cnvs.{library_kit}".format(
            library_kit=library_kit, **wildcards
        )
        yield "calls", [
            "work/{name_pattern}.{shard}/out/{name_pattern}.{shard}/.done".format(
                name_pattern=name_pattern, shard=shard
            )
            for shard in shards
        ]
        ext = "ploidy"
        name_pattern = "{mapper}.gcnv_contig_ploidy.{library_kit}".format(
            library_kit=library_kit, **wildcards
        )
        yield ext, "work/{name_pattern}/out/{name_pattern}/.done".format(name_pattern=name_pattern)

    @dictify
    def _get_input_files_post_germline_calls_case_mode(self, wildcards):
        """Yield input files for ``post_germline_calls`` in CASE mode.

        :param wildcards: Snakemake wildcards associated with rule, namely: 'mapper' (e.g., 'bwa')
        and 'library_name' (e.g., 'P001-N1-DNA1-WGS1').
        :type wildcards: snakemake.io.Wildcards
        """
        # Get library kit associated with library
        library_kit = self.ngs_library_to_kit.get(wildcards.library_name)

        # Get shards - based on scattered step
        model_dir_dict = None
        for model in self.config["gcnv"]["precomputed_model_paths"]:
            # Adjust library name to wildcard
            library_to_wildcard = model.get("library").strip().replace(" ", "_")
            if library_to_wildcard == library_kit:
                pattern = model.get("model_pattern")
                model_dir_dict = self._get_model_dir_to_dict(pattern)
                break
        # Yield cnv calls output
        name_pattern = "{mapper}.gcnv_call_cnvs_case_mode.{library_kit}".format(
            library_kit=library_kit, **wildcards
        )
        yield "calls", [
            "work/{name_pattern}.{shard}/out/{name_pattern}.{shard}/.done".format(
                name_pattern=name_pattern, shard=shard
            )
            for shard in model_dir_dict
        ]

        # Yield contig-ploidy output
        ext = "ploidy"
        name_pattern = "{mapper}.gcnv_contig_ploidy_case_mode.{library_kit}".format(
            library_kit=library_kit, **wildcards
        )
        yield ext, "work/{name_pattern}/out/{name_pattern}/.done".format(name_pattern=name_pattern)

    @listify
    def _get_input_files_merge_cohort_vcfs(self, wildcards):
        for lib in sorted(self.index_ngs_library_to_donor):
            if self.ngs_library_to_kit.get(lib) == wildcards.library_kit:
                name_pattern = "{mapper}.gcnv_post_germline_calls.{library_name}".format(
                    mapper=wildcards.mapper, library_name=lib
                )
                yield "work/{name_pattern}/out/{name_pattern}.vcf.gz".format(
                    name_pattern=name_pattern
                )

    @dictify
    def _get_input_files_extract_ped(self, wildcards):
        library_kit = self.ngs_library_to_kit[wildcards.library_name]
        name_pattern = "{mapper}.gcnv_merge_cohort_vcfs.{library_kit}".format(
            library_kit=library_kit, **wildcards
        )
        for key, ext in (("vcf", ".vcf.gz"), ("tbi", ".vcf.gz.tbi")):
            yield key, "work/{name_pattern}/out/{name_pattern}{ext}".format(
                name_pattern=name_pattern, ext=ext
            )

    def get_ped_members(self, wildcards):
        pedigree = self.index_ngs_library_to_pedigree[wildcards.library_name]
        return " ".join(
            donor.dna_ngs_library.name for donor in pedigree.donors if donor.dna_ngs_library
        )

    def get_output_files(self, action):
        """Get output function for gCNV build model rule.

        :param action: Action (i.e., step) in the workflow.
        :type action: str

        :return: Returns output function for gCNV rule based on inputted action.

        :raises UnsupportedActionException: if action not in class defined list of valid actions.
        """
        # Validate inputted action
        if action not in self.actions:
            valid_actions_str = ", ".join(self.actions)
            error_message = "Action '{action}' is not supported. Valid options: {options}".format(
                action=action, options=valid_actions_str
            )
            raise UnsupportedActionException(error_message)
        return getattr(self, "_get_output_files_{}".format(action))()

    @staticmethod
    @dictify
    def _get_output_files_preprocess_intervals():
        ext = "interval_list"
        name_pattern = "gcnv_preprocess_intervals.{library_kit}"
        yield ext, "work/{name_pattern}/out/{name_pattern}.{ext}".format(
            name_pattern=name_pattern, ext=ext
        )

    @staticmethod
    @dictify
    def _get_output_files_annotate_gc():
        ext = "tsv"
        name_pattern = "gcnv_annotate_gc.{library_kit}"
        yield ext, "work/{name_pattern}/out/{name_pattern}.{ext}".format(
            name_pattern=name_pattern, ext=ext
        )

    @staticmethod
    @dictify
    def _get_output_files_filter_intervals():
        ext = "interval_list"
        name_pattern = "{mapper}.gcnv_filter_intervals.{library_kit}"
        yield ext, "work/{name_pattern}/out/{name_pattern}.{ext}".format(
            name_pattern=name_pattern, ext=ext
        )

    @staticmethod
    def _get_output_files_scatter_intervals():
        return "work/{name_pattern}/out/{name_pattern}".format(
            name_pattern="{mapper}.gcnv_scatter_intervals.{library_kit}"
        )

    @staticmethod
    @dictify
    def _get_output_files_coverage():
        ext = "tsv"
        name_pattern = "{mapper}.gcnv_coverage.{library_name}"
        yield ext, "work/{name_pattern}/out/{name_pattern}.{ext}".format(
            name_pattern=name_pattern, ext=ext
        )

    @staticmethod
    @dictify
    def _get_output_files_contig_ploidy():
        """Yield dictionary with output files for ``contig_ploidy`` rule in COHORT MODE."""
        ext = "done"
        name_pattern = "{mapper}.gcnv_contig_ploidy.{library_kit}"
        yield ext, touch(
            "work/{name_pattern}/out/{name_pattern}/.{ext}".format(
                name_pattern=name_pattern, ext=ext
            )
        )

    @staticmethod
    @dictify
    def _get_output_files_contig_ploidy_case_mode():
        """Yield dictionary with output files for ``contig_ploidy`` rule in CASE MODE."""
        ext = "done"
        name_pattern = "{mapper}.gcnv_contig_ploidy_case_mode.{library_kit}"
        yield ext, touch(
            "work/{name_pattern}/out/{name_pattern}/.{ext}".format(
                name_pattern=name_pattern, ext=ext
            )
        )

    @staticmethod
    @dictify
    def _get_output_files_call_cnvs_cohort_mode():
        """Yield dictionary with output files for ``call_cnvs`` rle in COHORT MODE."""
        ext = "done"
        name_pattern = "{mapper}.gcnv_call_cnvs.{library_kit}.{shard}"
        yield ext, touch(
            "work/{name_pattern}/out/{name_pattern}/.{ext}".format(
                name_pattern=name_pattern, ext=ext
            )
        )

    @staticmethod
    @dictify
    def _get_output_files_call_cnvs_case_mode():
        """Yield dictionary with output files for ``call_cnvs`` rule in CASE MODE."""
        ext = "done"
        name_pattern = "{mapper}.gcnv_call_cnvs_case_mode.{library_kit}.{shard}"
        yield ext, touch(
            "work/{name_pattern}/out/{name_pattern}/.{ext}".format(
                name_pattern=name_pattern, ext=ext
            )
        )

    @staticmethod
    @dictify
    def _get_output_files_post_germline_calls():
        name_pattern = "{mapper}.gcnv_post_germline_calls.{library_name}"
        pairs = {"ratio_tsv": ".ratio.tsv", "itv_vcf": ".interval.vcf.gz", "seg_vcf": ".vcf.gz"}
        for key, ext in pairs.items():
            yield key, touch(
                "work/{name_pattern}/out/{name_pattern}{ext}".format(
                    name_pattern=name_pattern, ext=ext
                )
            )

    @staticmethod
    @dictify
    def _get_output_files_merge_cohort_vcfs():
        name_pattern = "{mapper}.gcnv_merge_cohort_vcfs.{library_kit}"
        pairs = {
            "vcf": ".vcf.gz",
            "vcf_md5": ".vcf.gz.md5",
            "tbi": ".vcf.gz.tbi",
            "tbi_md5": ".vcf.gz.tbi.md5",
        }
        for key, ext in pairs.items():
            yield key, "work/{name_pattern}/out/{name_pattern}{ext}".format(
                name_pattern=name_pattern, ext=ext
            )

    @staticmethod
    @dictify
    def _get_output_files_extract_ped():
        name_pattern = "{mapper}.gcnv.{library_name}"
        kvs = (
            ("vcf", ".vcf.gz"),
            ("vcf_md5", ".vcf.gz.md5"),
            ("tbi", ".vcf.gz.tbi"),
            ("tbi_md5", ".vcf.gz.tbi.md5"),
        )
        for key, suffix in kvs:
            yield key, "work/{name_pattern}/out/{name_pattern}{suffix}".format(
                name_pattern=name_pattern, suffix=suffix
            )

    @staticmethod
    def get_log_file(action):
        """Get log file.

        :param action: Action (i.e., step) in the workflow, examples: 'filter_intervals',
        'coverage', 'extract_ped'.
        :type action: str

        :return: Returns template path to log file.
        """
        if action in ("preprocess_intervals", "annotate_gc"):
            name_pattern = "gcnv_{action}.{{library_kit}}".format(action=action)
            return "work/{name_pattern}/log/{name_pattern}.log".format(name_pattern=name_pattern)
        elif action in (
            "filter_intervals",
            "contig_ploidy",
            "contig_ploidy_case_mode",
            "scatter_intervals",
            "merge_cohort_vcfs",
        ):
            name_pattern = "{{mapper}}.gcnv_{action}.{{library_kit}}".format(action=action)
            return "work/{name_pattern}/log/{name_pattern}.log".format(name_pattern=name_pattern)
        elif action.startswith("call_cnvs"):
            name_pattern = "{{mapper}}.gcnv_{action}.{{library_kit}}.{{shard}}".format(
                action=action
            )
            return "work/{name_pattern}/log/{name_pattern}.log".format(name_pattern=name_pattern)
        else:
            name_pattern = "{{mapper}}.gcnv_{action}.{{library_name}}".format(action=action)
            return "work/{name_pattern}/log/{name_pattern}.log".format(name_pattern=name_pattern)

    def get_cnv_model_result_files(self, _unused=None):
        """Get gCNV model results.
        :return: Returns list of result files for the gCNV build model sub-workflow.
        """
        # Initialise variables
        chosen_kits = []
        name_pattern_contig_ploidy = (
            "work/{mapper}.gcnv_contig_ploidy.{library_kit}/out/"
            "{mapper}.gcnv_contig_ploidy.{library_kit}/.done"
        )
        name_pattern_filter_intervals = (
            "work/{mapper}.gcnv_filter_intervals.{library_kit}/out/"
            "{mapper}.gcnv_filter_intervals.{library_kit}.interval_list"
        )

        # Get run mode
        analysis_mode = self.get_analysis_type()

        # Get list of library kits and donors to use.
        library_kits, _, kit_counts = self.parent.pick_kits_and_donors()

        if "gcnv" in self.config["tools"]:
            if analysis_mode == "cohort_mode":
                chosen_kits = [
                    kit for kit in library_kits if kit_counts.get(kit, 0) > MIN_KIT_SAMPLES
                ]
            elif analysis_mode == "case_mode_build":
                chosen_kits = list(library_kits)

            yield from expand(
                name_pattern_contig_ploidy,
                mapper=self.w_config["step_config"]["ngs_mapping"]["tools"]["dna"],
                library_kit=chosen_kits,
            )
            yield from expand(
                name_pattern_filter_intervals,
                mapper=self.w_config["step_config"]["ngs_mapping"]["tools"]["dna"],
                library_kit=chosen_kits,
            )

    def get_analysis_type(self):
        """Get analysis type.

        :return: Returns analysis type based on information provided on configuration file and
        sample sheets, either 'cohort_mode' or 'case_mode'.
        """
        return self.analysis_type

    def update_cluster_config(self, cluster_config):
        """Update cluster configuration for gCNV CNV calling"""
        high_resource_action = [
            "call_cnvs_cohort_mode",
            "call_cnvs_case_mode",
            "post_germline_calls",
            "post_germline_calls_cohort_mode",
            "post_germline_calls_case_mode",
        ]
        for action in self.actions:
            if action in high_resource_action:
                cluster_config["targeted_seq_cnv_calling_gcnv_{}".format(action)] = {
                    "mem": 12 * int(3.75 * 1024),
                    "time": "48:00",
                    "ntasks": 16,
                }
            else:
                cluster_config["targeted_seq_cnv_calling_gcnv_{}".format(action)] = {
                    "mem": 2 * int(3.75 * 1024),
                    "time": "04:00",
                    "ntasks": 1,
                }


class TargetedSeqCnvCallingWorkflow(BaseStep):
    """Perform germline targeted sequencing CNV calling"""

    #: Workflow name
    name = "targeted_seq_cnv_calling"

    sheet_shortcut_class = GermlineCaseSheet

    def __init__(
        self, workflow, config, cluster_config, config_lookup_paths, config_paths, workdir
    ):
        super().__init__(
            workflow,
            config,
            cluster_config,
            config_lookup_paths,
            config_paths,
            workdir,
            (NgsMappingWorkflow,),
        )
        # Register sub step classes so the sub steps are available
        self.register_sub_step_classes((XhmmStepPart, GcnvStepPart, LinkOutStepPart))
        # Register sub workflows
        self.register_sub_workflow("ngs_mapping", self.config["path_ngs_mapping"])
        # Build mapping from NGS DNA library to library kit.
        sub_step_name = "xhmm" if "xhmm" in self.config["tools"] else "gcnv"
        self.ngs_library_to_kit = self.sub_steps[sub_step_name].ngs_library_to_kit
        # Build dictionary with sample count per library kit
        _, _, self.library_kit_counts_dict = self.pick_kits_and_donors()

    @classmethod
    def default_config_yaml(cls):
        """Default configuration.

        :return: Returns default config YAML, to be overwritten by project-specific one.
        """
        return DEFAULT_CONFIG

    def get_library_count(self, library_kit):
        """Get library count.

        :param library_kit: Library kit name.
        :type library_kit: str

        :return: Returns number of samples with inputted library kit. If library name not defined,
        it returns zero.
        """
        return self.library_kit_counts_dict.get(library_kit, 0)

    @listify
    def all_donors(self, include_background=True):
        """Get all donors.

        :param include_background: Boolean flag to defined if background should be included or not.
        Default: True, i.e., background will be included.

        :return: Returns list of all donors in sample sheet.
        """
        sheets = self.shortcut_sheets
        if not include_background:
            sheets = list(filter(is_not_background, sheets))
        for sheet in sheets:
            for pedigree in sheet.cohort.pedigrees:
                yield from pedigree.donors

    @listify
    def all_background_donors(self):
        """Get all background donors.

        :return: Returns list of all background donors in sample sheets.
        """
        sheets = deepcopy(self.shortcut_sheets)
        sheets = list(filter(is_background, sheets))
        for sheet in sheets:
            for pedigree in sheet.cohort.pedigrees:
                yield from pedigree.donors

    @listify
    def get_result_files(self):
        """Return list of result files for the germline targeted sequencing CNV calling workflow.

        If xhmm/path_target_interval_list_mapping is non-empty then we will use this mapping.  In
        this case, only the samples that have a ``libraryKit`` set with a matching entry in the
        mapping. Otherwise, we will create output files for the input primary DNA library of all
        donors.
        """
        # Get list of library kits and donors to use.
        library_kits, donors, kit_counts = self.pick_kits_and_donors()
        # Actually yield the result files.
        name_pattern = "{mapper}.{caller}.{index.dna_ngs_library.name}"
        callers = ("xhmm", "gcnv")
        cnv_tools = [t for t in self.config["tools"] if t in callers]
        yield from self._yield_result_files(
            os.path.join("output", name_pattern, "out", name_pattern + "{ext}"),
            donors,
            mapper=self.w_config["step_config"]["ngs_mapping"]["tools"]["dna"],
            caller=cnv_tools,
            ext=EXT_VALUES,
        )
        if "xhmm" in self.config["tools"]:
            name_pattern = "{mapper}.xhmm_genotype.{library_kit}"
            chosen_kits = [kit for kit in library_kits if kit_counts.get(kit, 0) > MIN_KIT_SAMPLES]
            yield from expand(
                os.path.join("output", name_pattern, "out", name_pattern + "{ext}"),
                mapper=self.w_config["step_config"]["ngs_mapping"]["tools"]["dna"],
                caller=["xhmm"],
                library_kit=chosen_kits,
                ext=EXT_VALUES,
            )
        if "gcnv" in self.config["tools"]:
            name_pattern = "{mapper}.gcnv_merge_cohort_vcfs.{library_kit}"
            chosen_kits = [kit for kit in library_kits if kit_counts.get(kit, 0) > MIN_KIT_SAMPLES]
            yield from expand(
                os.path.join("output", name_pattern, "out", name_pattern + "{ext}"),
                mapper=self.w_config["step_config"]["ngs_mapping"]["tools"]["dna"],
                caller=["gcnv"],
                library_kit=chosen_kits,
                ext=EXT_VALUES,
            )

    def pick_kits_and_donors(self):
        """Return ``(library_kits, donors)`` with the donors with a matching kit and the kits with a
        matching donor.
        """
        kit_counts = {name: 0 for name in self.ngs_library_to_kit.values()}
        for name in self.ngs_library_to_kit.values():
            kit_counts[name] += 1
        donors = [
            donor
            for donor in self.all_donors()
            if donor.dna_ngs_library and donor.dna_ngs_library.name in self.ngs_library_to_kit
        ]
        return list(sorted(set(self.ngs_library_to_kit.values()))), donors, kit_counts

    def _yield_result_files(self, tpl, donors, **kwargs):
        """Build output paths from path template and extension list.

        Will only yield the result files for pedigrees where the index is in ``donors``.
        """
        donor_names = {donor.name for donor in donors}
        for sheet in filter(is_not_background, self.shortcut_sheets):
            for pedigree in sheet.cohort.pedigrees:
                if pedigree.index.name in donor_names:
                    yield from expand(tpl, index=[pedigree.index], **kwargs)

    def check_config(self):
        """Check that the necessary configuration is available for the step"""
        self.ensure_w_config(
            config_keys=("step_config", "targeted_seq_cnv_calling", "path_ngs_mapping"),
            msg="Path to NGS mapping not configured but required for targeted seq. CNV calling",
        )
