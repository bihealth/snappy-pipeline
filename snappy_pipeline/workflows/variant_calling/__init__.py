# -*- coding: utf-8 -*-
"""Implementation of the ``variant_calling`` step

The ``variant_calling`` step takes as the input the results of the ``ngs_mapping`` step
(aligned reads in BAM format) and performs germline variant calling.  The result are variant files
with germline variants (bgzip-ed and indexed VCF files).

Usually, the variant calling step is followed by the ``variant_annotation`` step.

==========
Stability
==========

The HaplotypeCaller and UnifiedGenotyper from the Genome Analysis Toolkit (GATK) are considered
stable.

The other supported callers are still in the experimental stage and may not be stable.

==========
Step Input
==========

The variant calling step uses Snakemake sub workflows for using the result of the ``ngs_mapping``
step.

===========
Step Output
===========

For all pedigrees, variant calling will be performed on the primary DNA NGS libraries of all
members, separately for each configured read mapper and variant caller.  The name of the primary
DNA NGS library of the index will be used as an identification token in the output file.  For each
read mapper, variant caller, and pedigree, the following files will be generated:

- ``{mapper}.{var_caller}.{lib_name}.vcf.gz``
- ``{mapper}.{var_caller}.{lib_name}.vcf.gz.tbi``
- ``{mapper}.{var_caller}.{lib_name}.vcf.gz.md5``
- ``{mapper}.{var_caller}.{lib_name}.vcf.gz.tbi.md5``

For example, it might look as follows for the example from above:

::

    output/
    +-- bwa.freebayes.P001-N1-DNA1-WES1
    |   `-- out
    |       |-- bwa.freebayes.P001-N1-DNA1-WES1.vcf.gz
    |       |-- bwa.freebayes.P001-N1-DNA1-WES1.vcf.gz.tbi
    |       |-- bwa.freebayes.P001-N1-DNA1-WES1.vcf.gz.md5
    |       `-- bwa.freebayes.P001-N1-DNA1-WES1.vcf.gz.tbi.md5
    [...]

Generally, these files will be unfiltered, i.e., contain low-quality variants.

====================
Global Configuration
====================

- If GATK HaplotypeCaller or GATK UnifiedGenotyper are activated then
  ``static_data_config/dbsnp/path`` must be properly configured
- ``static_data_config/reference/path`` must be set appropriately

=====================
Default Configuration
=====================

The default configuration is as follows.

.. include:: DEFAULT_CONFIG_variant_calling.rst

==================================
Available Germline Variant Callers
==================================

The following germline variant callers are currently available

- ``"bcftools"``  -- samtools mpileup plus bcftools
- ``"freebayes"``
- ``"gatk_hc"`` -- GATK HaplotypeCaller
- ``"gatk_hc_gvcf"`` -- GATK HaplotypeCaller via GVCF files
- ``"gatk_ug"`` -- GATK UnifiedGenotyper
- ``"platypus"``

=======
Reports
=======

Currently, the following reports are generated (and are linked from the output directory):

- bcftools_stats (txt) is always generated by default.
  Within this file, stats are broken down separately for known and novel events.
  Report contents depend on the version of bcftools used. With version 1.3.1 the report includes
  the following details:

    - SN, Summary numbers
    - TSTV, Transitions/transversions
    - SiS, Singleton stats
    - AF, Stats by non-reference allele frequency
    - QUAL, Stats by quality
    - IDD, InDel distribution
    - ST, Substitution types
    - DP, Depth distribution
    - PSC, Per-sample counts
    - PSI, Per-Sample indels
    - HWE, Hardy-Weinberg equilibrium

- jannovar_statistics (txt) is always generated by default.
  Requires jannovar_statistics/path_ser to be set to a ".ser" file.
  Using jannovar-cli and htslib version 1.3.2 the report includes the following details:

     - putative_impacts (counts by type)
     - variant_effects (counts by type)
     - genome_regions (counts by type)
     - ts_tv_count (count TS, count TV)
     - alt_allele_count (counts for each number of alleles)
     - filter_count (counts by type, if any)
     - is_filtered_count (count passed and failed, if filtering is used)
     - contig_counts (count events per chromosome)

.. _variant_calling_parallel_execution:

==================
Parallel Execution
==================

For many of the variant callers, cluster-parallel execution has been implemented (indicated by
having a ``drmaa_snippet`` configuration setting).  Here, a temporary directory with a Snakemake
workflow is written out and then executed.  The default behaviour that the temporary files are
removed in the case of an error.  This behaviour can be changed by setting the ``keep_tmpdir``
setting to ``"onerror"`` or ``"always"``.  Further, for debugging, the number of windows to
create can be limited using ``debug_trunc_tokens`` (the default of ``0``) leads to the processing
of all windows.  Resource requirements in terms of memory or running time can be boosted using
``job_mult_memory`` and ``job_mult_time`` (similarly for the joining step and ``merge_mult_*``).

When the temporary directory is kept, a failed execution can be restarted by calling ``snakemake``
in the temporary directory with the command line written to the file ``snakemake_call.sh``.
"""

from collections import OrderedDict
import os
import os.path
import sys

from biomedsheets.shortcuts import GermlineCaseSheet, is_not_background
from snakemake.io import expand

from snappy_pipeline.utils import dictify, listify
from snappy_pipeline.workflows.abstract import (
    BaseStep,
    BaseStepPart,
    LinkOutStepPart,
    WritePedigreeStepPart,
)
from snappy_pipeline.workflows.ngs_mapping import NgsMappingWorkflow
from snappy_wrappers.tools.genome_windows import yield_regions

__author__ = "Manuel Holtgrewe <manuel.holtgrewe@bihealth.de>"

#: Extensions of files to create as main payload
EXT_VALUES = (".vcf.gz", ".vcf.gz.tbi", ".vcf.gz.md5", ".vcf.gz.tbi.md5")

#: Names of the files to create for the extension
EXT_NAMES = ("vcf", "tbi", "vcf_md5", "tbi_md5")

#: Available germline variant callers
VARIANT_CALLERS = (
    "bcftools",
    "freebayes",
    "gatk_hc",
    "gatk_hc_gvcf",
    "gatk_ug",
    "platypus",
    "varscan",
)

#: Callers that support cohort-wide calling.
COHORT_WIDE_CALLERS = ("gatk_hc_gvcf", "varscan")

#: Default configuration for the variant_calling step
DEFAULT_CONFIG = r"""
# Default configuration variant_calling
step_config:
  variant_calling:
    drmaa_snippet: ''  # default, you can override by step below
    path_ngs_mapping: ../ngs_mapping  # REQUIRED
    tools: ['gatk_ug']
    jannovar_statistics:
      path_ser: REQUIRED  # REQUIRED
    platypus:
      num_threads: 16
      ignore_chroms:            # patterns of chromosome names to ignore
      - NC_007605  # herpes virus
      - hs37d5     # GRCh37 decoy
      - chrEBV     # Eppstein-Barr Virus
      - '*_decoy'  # decoy contig
      - 'HLA-*'    # HLA genes
    bcftools:
      max_depth: 4000
      max_indel_depth: 4000
      window_length: 10000000
      num_threads: 16
      ignore_chroms:            # patterns of chromosome names to ignore
      - NC_007605  # herpes virus
      - hs37d5     # GRCh37 decoy
      - chrEBV     # Eppstein-Barr Virus
      - '*_decoy'  # decoy contig
      - 'HLA-*'    # HLA genes
    freebayes:
      use_standard_filters: true
      window_length: 10000000
      num_threads: 16
      ignore_chroms:            # patterns of chromosome names to ignore
      - NC_007605  # herpes virus
      - hs37d5     # GRCh37 decoy
      - chrEBV     # Eppstein-Barr Virus
      - '*_decoy'  # decoy contig
      - 'HLA-*'    # HLA genes
    gatk_hc:
      # Parallelization configuration
      drmaa_snippet: ''         # value to pass in as additional DRMAA arguments
      num_threads: 2            # number of cores to use locally
      window_length: 5000000    # split input into windows of this size, each triggers a job
      num_jobs: 500             # number of windows to process in parallel
      use_drmaa: true           # use DRMAA for parallel processing
      restart_times: 0          # number of times to re-launch jobs in case of failure
      max_jobs_per_second: 10   # throttling of job creation
      max_status_checks_per_second: 10  # throttling of status jobs
      debug_trunc_tokens: 0     # truncation to first N tokens (0 for none)
      keep_tmpdir: never        # keep temporary directory, {always, never, onerror}
      job_mult_memory: 1        # memory multiplier
      job_mult_time: 1          # running time multiplier
      merge_mult_memory: 1      # memory multiplier for merging
      merge_mult_time: 1        # running time multiplier for merging
      ignore_chroms:            # patterns of chromosome names to ignore
      - NC_007605  # herpes virus
      - hs37d5     # GRCh37 decoy
      - chrEBV     # Eppstein-Barr Virus
      - '*_decoy'  # decoy contig
      - 'HLA-*'    # HLA genes
      # GATK HC--specific configuration
      allow_seq_dict_incompatibility: false
      annotations:
      - BaseQualityRankSumTest
      - FisherStrand
      - GCContent
      - HaplotypeScore
      - HomopolymerRun
      - MappingQualityRankSumTest
      - MappingQualityZero
      - QualByDepth
      - ReadPosRankSumTest
      - RMSMappingQuality
      - DepthPerAlleleBySample
      - Coverage
      - ClippingRankSumTest
      - DepthPerSampleHC
    gatk_hc_gvcf:
      # Enable cohort-wide calling
      cohort_wide: true
      # Enable pedigree-wise calling
      pedigree_wise: true
      # Parallelization configuration
      drmaa_snippet: ''         # value to pass in as additional DRMAA arguments
      num_threads: 2            # number of cores to use locally
      window_length: 5000000    # split input into windows of this size, each triggers a job
      num_jobs: 500             # number of windows to process in parallel
      num_jobs_combine_gvcf_cort: 0
      num_jobs_genotype_cohort: 0
      use_drmaa: true           # use DRMAA for parallel processing
      restart_times: 10         # number of times to re-launch jobs in case of failure
      max_jobs_per_second: 10   # throttling of job creation
      max_status_checks_per_second: 10  # throttling of status jobs
      ignore_chroms:            # patterns of chromosome names to ignore
      - NC_007605  # herpes virus
      - hs37d5     # GRCh37 decoy
      - chrEBV     # Eppstein-Barr Virus
      - '*_decoy'  # decoy contig
      - 'HLA-*'    # HLA genes
      # GATK HC--specific configuration
      allow_seq_dict_incompatibility: false
      annotations:
      - BaseQualityRankSumTest
      - FisherStrand
      - GCContent
      - HaplotypeScore
      - HomopolymerRun
      - MappingQualityRankSumTest
      - MappingQualityZero
      - QualByDepth
      - ReadPosRankSumTest
      - RMSMappingQuality
      - DepthPerAlleleBySample
      - Coverage
      - ClippingRankSumTest
      - DepthPerSampleHC
    gatk_ug:
      # Parallelization configuration
      drmaa_snippet: ''         # value to pass in as additional DRMAA arguments
      num_threads: 2            # number of cores to use locally
      window_length: 5000000    # split input into windows of this size, each triggers a job
      num_jobs: 500             # number of windows to process in parallel
      use_drmaa: true           # use DRMAA for parallel processing
      restart_times: 0          # number of times to re-launch jobs in case of failure
      max_jobs_per_second: 10   # throttling of job creation
      max_status_checks_per_second: 10  # throttling of status jobs
      debug_trunc_tokens: 0     # truncation to first N tokens (0 for none)
      keep_tmpdir: never        # keep temporary directory, {always, never, onerror}
      job_mult_memory: 1        # memory multiplier
      job_mult_time: 1          # running time multiplier
      merge_mult_memory: 1      # memory multiplier for merging
      merge_mult_time: 1        # running time multiplier for merging
      ignore_chroms:            # patterns of chromosome names to ignore
      - NC_007605  # herpes virus
      - hs37d5     # GRCh37 decoy
      - chrEBV     # Eppstein-Barr Virus
      - '*_decoy'  # decoy contig
      - 'HLA-*'    # HLA genes
      # GATK UG--specific configuration
      allow_seq_dict_incompatibility: false
      downsample_to_coverage: 250
      annotations:
      - BaseQualityRankSumTest
      - FisherStrand
      - GCContent
      - HaplotypeScore
      - HomopolymerRun
      - MappingQualityRankSumTest
      - MappingQualityZero
      - QualByDepth
      - ReadPosRankSumTest
      - RMSMappingQuality
      - DepthPerAlleleBySample
      - Coverage
      - ClippingRankSumTest
      - DepthPerSampleHC
    # Configuration for cohort-wide variant calling using Varscan.
    varscan:
      # TODO: only cohort wide used and tested so far, settings should probably
      #       be switched around
      # Enable cohort-wide calling
      cohort_wide: true
      # Enable pedigree-wise calling
      pedigree_wise: false
      # Divisor for window length in case of cohort-wide
      cohort_window_divisor: 50
      # Parallelization configuration
      drmaa_snippet: ''         # value to pass in as additional DRMAA arguments
      num_threads: 2            # number of cores to use locally
      window_length: 5000000    # split input into windows of this size, each triggers a job
      num_jobs: 500             # number of windows to process in parallel
      use_drmaa: true           # use drmaa for parallel processing
      restart_times: 0          # number of times to re-launch jobs in case of failure
      max_jobs_per_second: 10   # throttling of job creation
      max_status_checks_per_second: 10  # throttling of status jobs
      debug_trunc_tokens: 0     # truncation to first N tokens (0 for none)
      keep_tmpdir: never        # keep temporary directory, {always, never, onerror}
      job_mult_memory: 1        # memory multiplier
      job_mult_time: 1          # running time multiplier
      merge_mult_memory: 1      # memory multiplier for merging
      merge_mult_time: 1        # running time multiplier for merging
      ignore_chroms:            # patterns of chromosome names to ignore
      - nc_007605  # herpes virus
      - hs37d5     # grch37 decoy
      - chrebv     # eppstein-barr virus
      - '*_decoy'  # decoy contig
      - 'hla-*'    # hla genes
      # Configuration for samtools mpileup
      max_depth: 4000
      max_indel_depth: 4000
      min_bq: 13
      no_baq: True
      # Configuration for Varscan
      min_coverage: 8
      min_reads2: 2
      min_avg_qual: 15
      min_var_freq: 0.01
      min_freq_for_hom: 0.75
      p_value: 99e-02
"""


class VariantCallingStepPart(BaseStepPart):
    """Base class for germline variant calling step parts

    Variant calling is performed on a per-pedigree level.  The (one) index individual is used
    for naming the output file.
    """

    def __init__(self, parent):
        super().__init__(parent)
        self.base_path_out = (
            "work/{{mapper}}.{var_caller}.{{index_library_name}}/out/"
            "{{mapper}}.{var_caller}.{{index_library_name}}{ext}"
        )
        self.base_path_tmp = self.base_path_out.replace("/out/", "/tmp/")
        # Build shortcut from index library name to pedigree
        self.index_ngs_library_to_pedigree = OrderedDict()
        for sheet in self.parent.shortcut_sheets:
            self.index_ngs_library_to_pedigree.update(sheet.index_ngs_library_to_pedigree)

    def get_input_files(self, action):
        @listify
        def input_function(wildcards):
            """Helper wrapper function"""
            # TODO: Actually, the structure should be {'ped': ..., 'bam': [], 'bai': []}
            # Get shorcut to NGS mapping sub workflow
            ngs_mapping = self.parent.sub_workflows["ngs_mapping"]
            # Get names of primary libraries of the selected pedigree.  The pedigree is selected
            # by the primary DNA NGS library of the index.
            pedigree = self.index_ngs_library_to_pedigree[wildcards.index_library_name]
            if not pedigree.index or not pedigree.index.dna_ngs_library:
                msg = "INFO: pedigree without index (names: {})"  # pragma: no cover
                donor_names = list(sorted(d.name for d in pedigree.donors))
                print(msg.format(donor_names), file=sys.stderr)  # pragma: no cover
                return
            if pedigree.index.dna_ngs_library:
                yield "work/write_pedigree.{library_name}/out/{library_name}.ped".format(
                    library_name=pedigree.index.dna_ngs_library.name, **wildcards
                )
            for donor in pedigree.donors:
                if donor.dna_ngs_library:
                    for ext in (".bam", ".bam.bai"):
                        tpl = "output/{mapper}.{library_name}/out/{mapper}.{library_name}{ext}"
                        yield ngs_mapping(
                            tpl.format(
                                library_name=donor.dna_ngs_library.name, ext=ext, **wildcards
                            )
                        )

        assert action == "run", "Unsupported actions"
        return input_function

    def get_output_files(self, action):
        """Return output files that all germline variant calling sub steps must
        return (VCF + TBI file)
        """
        assert action == "run"
        return dict(
            zip(EXT_NAMES, expand(self.base_path_out, var_caller=[self.name], ext=EXT_VALUES))
        )

    @dictify
    def _get_log_file(self, action):
        """Return dict of log files."""
        prefix = (
            "work/{{mapper}}.{caller}.{{index_library_name}}/log/"
            "{{mapper}}.{caller}.{{index_library_name}}"
        ).format(caller=self.__class__.name)
        key_ext = (
            ("log", ".log"),
            ("conda_info", ".conda_info.txt"),
            ("conda_list", ".conda_list.txt"),
        )
        for key, ext in key_ext:
            yield key, prefix + ext


class BcftoolsStepPart(VariantCallingStepPart):
    """Germline variant calling with bcftools"""

    name = "bcftools"

    def update_cluster_config(self, cluster_config):
        cluster_config["variant_calling_bcftools_run"] = {
            "mem": int(3.75 * 1024 * 16),
            "time": "48:00",
            "ntasks": 16,
        }


class FreebayesStepPart(VariantCallingStepPart):
    """Germline variant calling with freebayes"""

    name = "freebayes"

    def update_cluster_config(self, cluster_config):
        cluster_config["variant_calling_freebayes_run"] = {
            "mem": int(3.75 * 1024 * 16),
            "time": "48:00",
            "ntasks": 16,
        }


class GatkCallerStepPartBase(VariantCallingStepPart):
    """Germlin variant calling with GATK caller"""

    def check_config(self):
        if self.__class__.name not in self.config["tools"]:
            return  # caller not enabled, skip  # pragma: no cover
        self.parent.ensure_w_config(
            ("static_data_config", "dbsnp", "path"),
            "dbSNP not configured but required for {}".format(self.__class__.name),
        )

    def update_cluster_config(self, cluster_config):
        cluster_config["variant_calling_{}_run".format(self.__class__.name)] = {
            "mem": 14 * 1024,
            "time": "80:00",
            "ntasks": 1,
        }


class GatkHaplotypeCallerStepPart(GatkCallerStepPartBase):
    """Germline variant calling with GATK HaplotypeCaller"""

    name = "gatk_hc"


class GatkUnifiedGenotyperStepPart(GatkCallerStepPartBase):
    """Germline variant calling with GATK UnifiedGenotyper"""

    name = "gatk_ug"


class PlatypusStepPart(VariantCallingStepPart):
    """Germline variant calling with Platypus"""

    name = "platypus"

    def update_cluster_config(self, cluster_config):
        cluster_config["variant_calling_platypus_run"] = {
            "mem": int(3.75 * 1024 * 16),
            "time": "20:00",
            "ntasks": 16,
        }


class GatkHaplotypeCallerGvcfStepPart(BaseStepPart):
    """Base class for germline variant calling step parts

    Variant calling is performed on a per-pedigree level.  The (one) index individual is used
    for naming the output file.
    """

    name = "gatk_hc_gvcf"

    #: Actions in GATK HC GVCF workflow
    actions = ("discover", "genotype_pedigree", "combine_gvcf", "genotype_cohort")

    #: Directory infixes
    dir_infixes = {
        "discover": "{mapper}.gatk_hc_gvcf.discover.{library_name}",
        "genotype_pedigree": "{mapper}.gatk_hc_gvcf.{index_library_name,[^\.]+}",
        "combine_gvcf": "{mapper}.gatk_hc_gvcf.combine_gvcf",
        "genotype_cohort": "{mapper}.gatk_hc_gvcf.whole_cohort",
    }

    def __init__(self, parent):
        super().__init__(parent)
        self.base_path_out = (
            "work/{{mapper}}.{var_caller}.{{ngs_library}}/out/"
            "{{mapper}}.{var_caller}.{{ngs_library}}{ext}"
        )
        self.base_path_tmp = self.base_path_out.replace("/out/", "/tmp/")
        # Build shortcut from index library name to pedigree
        self.index_ngs_library_to_pedigree = OrderedDict()
        for sheet in self.parent.shortcut_sheets:
            self.index_ngs_library_to_pedigree.update(sheet.index_ngs_library_to_pedigree)

    def get_input_files(self, action):
        """Return appropriate input function for the given action"""
        assert action in self.actions
        mapping = {
            "discover": self._get_input_files_discover,
            "genotype_pedigree": self._get_input_files_genotype_pedigree,
            "combine_gvcf": self._get_input_files_combine_gvcf,
            "genotype_cohort": self._get_input_files_genotype_cohort,
        }
        return mapping[action]

    @listify
    def _get_input_files_discover(self, wildcards):
        """Return input files for "discover" action"""
        ngs_mapping = self.parent.sub_workflows["ngs_mapping"]
        for ext in (".bam", ".bam.bai"):
            tpl = "output/{mapper}.{library_name}/out/{mapper}.{library_name}{ext}"
            yield ngs_mapping(tpl.format(ext=ext, **wildcards))

    @listify
    def _get_input_files_genotype_pedigree(self, wildcards):
        """Return input files for "genotype_pedigree" action"""
        pedigree = self.index_ngs_library_to_pedigree[wildcards.index_library_name]
        for donor in pedigree.donors:
            paths = self.get_output_files("discover")
            yield paths["vcf"].format(library_name=donor.dna_ngs_library.name, **wildcards)

    @listify
    def _get_input_files_combine_gvcf(self, wildcards):
        """Return input files for "combine_gvcf" action"""
        for sheet in self.parent.shortcut_sheets:
            for donor in sheet.donors:
                paths = self.get_output_files("discover")
                yield paths["vcf"].format(library_name=donor.dna_ngs_library.name, **wildcards)

    @listify
    def _get_input_files_genotype_cohort(self, wildcards):
        """Return input files for "genotype_cohort" action"""
        for path in self.get_output_files("combine_gvcf")["vcf"]:
            yield path.format(**wildcards)

    def get_args(self, action):
        """Return function that maps wildcards to dict for input files"""
        assert action == "combine_gvcf", "Unsupported actions"
        return {"genome_regions": self._get_args_gvcf_regions()}

    def _get_args_gvcf_regions(self):
        """Return list of regions to use for GVCF parallelization

        Returns ``OrderedDict`` with grouping by chromsome name.
        """
        fai_path = self.w_config["static_data_config"]["reference"]["path"] + ".fai"
        window_length = max(
            10 * 1000 * 1000, self.parent.config["gatk_hc_gvcf"]["window_length"] // 10
        )
        ignore_chroms = self.parent.config["gatk_hc_gvcf"]["ignore_chroms"]
        result = OrderedDict()
        with open(fai_path, "rt") as fai_file:
            for region in yield_regions(fai_file, window_length, ignore_chroms=ignore_chroms):
                # Note that we have to convert GenomeRegion to dict here via vars
                result.setdefault(region.chrom, []).append(vars(region))
        return result

    @dictify
    def get_output_files(self, action):
        """Return output files that all germline variant calling sub steps must
        return (VCF + TBI file)
        """
        assert action in self.actions
        if action != "combine_gvcf":
            for name, ext in {"vcf": ".vcf.gz", "tbi": ".vcf.gz.tbi"}.items():
                if action == "discover":
                    ext = ".g" + ext
                infix = self.dir_infixes[action].replace(r",[^\.]+", "")
                yield name, "work/" + infix + "/out/" + infix + ext
                yield name + "_md5", "work/" + infix + "/out/" + infix + ext + ".md5"
        else:
            result = OrderedDict()
            for chrom, _ in self._get_args_gvcf_regions().items():
                chrom = chrom.replace(".", "_")
                for name, ext in {"vcf": ".g.vcf.gz", "tbi": ".g.vcf.gz.tbi"}.items():
                    infix = self.dir_infixes[action].replace(r",[^\.]+", "")
                    result.setdefault(name, []).append(
                        "work/" + infix + "/out/" + infix + "." + chrom + ext
                    )
                    result.setdefault(name + "_md5", []).append(
                        "work/" + infix + "/out/" + infix + "." + chrom + ext + ".md5"
                    )
            yield from result.items()

    def get_log_file(self, action):
        assert action in self.actions
        infix = self.dir_infixes[action].replace(r",[^\.]+", "")
        return "work/" + infix + "/log/snakemake.log"

    def update_cluster_config(self, cluster_config):
        for action in self.actions:
            if action == "combine_gvcf":
                cluster_config["variant_calling_gatk_hc_gvcf_{action}".format(action=action)] = {
                    "mem": 10 * 1024,
                    "time": "240:00",
                    "ntasks": 1,
                }
            else:
                cluster_config["variant_calling_gatk_hc_gvcf_{action}".format(action=action)] = {
                    "mem": 10 * 1024,
                    "time": "80:00",
                    "ntasks": 1,
                }


class VarscanStepPart(BaseStepPart):
    """Variant calling using Varscan.

    Variants are called in a whole-pedigree or whole-cohort fashion.
    """

    name = "varscan"

    #: Actions in GATK HC GVCF workflow
    actions = ("call_pedigree", "call_cohort")

    #: Directory infixes
    dir_infixes = {
        "call_pedigree": "{mapper}.varscan.{index_library_name,[^\.]+}",
        "call_cohort": "{mapper}.varscan.whole_cohort",
    }

    def __init__(self, parent):
        super().__init__(parent)
        self.base_path_out = (
            "work/{{mapper}}.{var_caller}.{{ngs_library}}/out/"
            "{{mapper}}.{var_caller}.{{ngs_library}}{ext}"
        )
        self.base_path_tmp = self.base_path_out.replace("/out/", "/tmp/")
        # Build shortcut from index library name to pedigree
        self.index_ngs_library_to_pedigree = OrderedDict()
        for sheet in self.parent.shortcut_sheets:
            self.index_ngs_library_to_pedigree.update(sheet.index_ngs_library_to_pedigree)

    def get_input_files(self, action):
        """Return appropriate input function for the given action"""
        assert action in self.actions
        mapping = {
            "call_pedigree": self._get_input_files_call_pedigree,
            "call_cohort": self._get_input_files_call_cohort,
        }
        return mapping[action]

    @listify
    def _get_input_files_call_pedigree(self, wildcards):
        """Return input files for "call_pedigree" action"""
        ngs_mapping = self.parent.sub_workflows["ngs_mapping"]
        pedigree = self.index_ngs_library_to_pedigree[wildcards.index_library_name]
        for donor in pedigree.donors:
            for ext in (".bam", ".bam.bai"):
                tpl = "output/{mapper}.{library_name}/out/{mapper}.{library_name}{ext}"
                library_name = donor.dna_ngs_library.name
                yield ngs_mapping(tpl.format(ext=ext, library_name=library_name, **wildcards))

    @listify
    def _get_input_files_call_cohort(self, wildcards):
        """Return input files for "combine_gvcf" action"""
        ngs_mapping = self.parent.sub_workflows["ngs_mapping"]
        for library_name in sorted(self.index_ngs_library_to_pedigree.keys()):
            for ext in (".bam", ".bam.bai"):
                tpl = "output/{mapper}.{library_name}/out/{mapper}.{library_name}{ext}"
                yield ngs_mapping(tpl.format(ext=ext, library_name=library_name, **wildcards))

    @dictify
    def get_output_files(self, action):
        """Return output files that all germline variant calling sub steps must
        return (VCF + TBI file)
        """
        assert action in self.actions
        for name, ext in {"vcf": ".vcf.gz", "tbi": ".vcf.gz.tbi"}.items():
            infix = self.dir_infixes[action].replace(r",[^\.]+", "")
            yield name, "work/" + infix + "/out/" + infix + ext
            yield name + "_md5", "work/" + infix + "/out/" + infix + ext + ".md5"

    @dictify
    def _get_log_file(self, action):
        """Return dict of log files."""
        infix = self.dir_infixes[action].replace(r",[^\.]+", "")
        prefix = os.path.join("work", infix, "log", infix)
        key_ext = (
            ("log", ".log"),
            ("conda_info", ".conda_info.txt"),
            ("conda_list", ".conda_list.txt"),
        )
        for key, ext in key_ext:
            yield key, prefix + ext

    def update_cluster_config(self, cluster_config):
        for action in self.actions:
            if action == "call_pedigree":
                cluster_config["variant_calling_varscan_call_pedigree".format(action=action)] = {
                    "mem": 4 * 1024,
                    "time": "168:00",
                    "ntasks": 1,
                }
            else:
                cluster_config["variant_calling_varscan_call_cohort".format(action=action)] = {
                    "mem": 16 * 1024,
                    "time": "168:00",
                    "ntasks": 1,
                }


class BcftoolsStatsStepPart(BaseStepPart):
    """Base class for VCF statistics computation with "bcftools stats"

    Statistics are computed overall and per-sample
    """

    # TODO: maybe we need to use "--stats" anyway and can handle pedigree VCF files then...

    name = "bcftools_stats"

    def __init__(self, parent):
        super().__init__(parent)
        self.base_path_out = (
            "work/{mapper}.{var_caller}.{index_ngs_library}/report/bcftools_stats/"
            "{mapper}.{var_caller}.{index_ngs_library}.{donor_ngs_library}"
        )
        # Build shortcut from index library name to pedigree
        self.index_ngs_library_to_pedigree = OrderedDict()
        for sheet in self.parent.shortcut_sheets:
            self.index_ngs_library_to_pedigree.update(sheet.index_ngs_library_to_pedigree)

    @dictify
    def get_input_files(self, action):
        """Return path to input files"""
        assert action == "run", "Unsupported actions"
        # Return path to input VCF file
        yield "vcf", (
            "work/{mapper}.{var_caller}.{index_ngs_library}/out/"
            "{mapper}.{var_caller}.{index_ngs_library}.vcf.gz"
        )

    @dictify
    def get_output_files(self, action):
        """Return output files that all germline variant calling sub steps must return (VCF +
        TBI file)
        """
        assert action == "run"
        EXT_NAMES = {"txt": ".txt", "txt_md5": ".txt.md5"}
        for key, ext in EXT_NAMES.items():
            yield key, self.base_path_out + ext

    def get_log_file(self, action):
        assert action == "run"
        return (
            "work/{mapper}.{var_caller}.{index_ngs_library}/log/bcftools_stats/"
            "{mapper}.{var_caller}.{index_ngs_library}.{donor_ngs_library}.log"
        )

    def update_cluster_config(self, cluster_config):
        cluster_config["variant_calling_bcftools_stats_report"] = {
            "mem": 1024,
            "time": "02:00",
            "ntasks": 1,
        }


class JannovarStatisticsStepPart(BaseStepPart):
    """Base class for VCF statistics computation with "jannovar statistics"

    Statistics are computed overall and per-sample
    """

    name = "jannovar_statistics"

    def __init__(self, parent):
        super().__init__(parent)
        self.base_path_out = (
            "work/{mapper}.{var_caller}.{index_ngs_library}/report/jannovar_statistics/"
            "{mapper}.{var_caller}.{index_ngs_library}"
        )
        # Build shortcut from index library name to pedigree
        self.index_ngs_library_to_pedigree = OrderedDict()
        for sheet in self.parent.shortcut_sheets:
            self.index_ngs_library_to_pedigree.update(sheet.index_ngs_library_to_pedigree)

    @dictify
    def get_input_files(self, action):
        """Return path to input files"""
        assert action == "run", "Unsupported actions"
        # Return path to input VCF file
        yield "vcf", (
            "work/{mapper}.{var_caller}.{index_ngs_library}/out/"
            "{mapper}.{var_caller}.{index_ngs_library}.vcf.gz"
        )

    @dictify
    def get_output_files(self, action):
        """Return output files that all germline variant calling sub steps must return (VCF +
        TBI file)
        """
        assert action == "run"
        EXT_NAMES = {"report": ".txt", "report_md5": ".txt.md5"}
        for key, ext in EXT_NAMES.items():
            yield key, self.base_path_out + ext

    def get_log_file(self, action):
        assert action == "run"
        return (
            "work/{mapper}.{var_caller}.{index_ngs_library}/log/jannovar_statistics/"
            "{mapper}.{var_caller}.{index_ngs_library}.log"
        )

    def update_cluster_config(self, cluster_config):
        cluster_config["variant_calling_jannovar_statistics_report"] = {
            "mem": int(3.75 * 1024 * 2),
            "time": "04:00",
            "ntasks": 2,
        }


class VariantCallingWorkflow(BaseStep):
    """Perform germline variant calling"""

    name = "variant_calling"
    sheet_shortcut_class = GermlineCaseSheet

    @classmethod
    def default_config_yaml(cls):
        """Return default config YAML, to be overwritten by project-specific one"""
        return DEFAULT_CONFIG

    def __init__(
        self, workflow, config, cluster_config, config_lookup_paths, config_paths, workdir
    ):
        super().__init__(
            workflow,
            config,
            cluster_config,
            config_lookup_paths,
            config_paths,
            workdir,
            (NgsMappingWorkflow,),
        )
        # Register sub step classes so the sub steps are available
        self.register_sub_step_classes(
            (
                WritePedigreeStepPart,
                BcftoolsStepPart,
                FreebayesStepPart,
                GatkHaplotypeCallerStepPart,
                GatkHaplotypeCallerGvcfStepPart,
                GatkUnifiedGenotyperStepPart,
                PlatypusStepPart,
                VarscanStepPart,
                BcftoolsStatsStepPart,
                JannovarStatisticsStepPart,
                LinkOutStepPart,
            )
        )
        # Register sub workflows
        self.register_sub_workflow("ngs_mapping", self.config["path_ngs_mapping"])

    @listify
    def get_result_files(self):
        """Return list of result files for the NGS mapping workflow

        We will process all primary DNA libraries and perform joint calling within pedigrees
        """
        name_pattern = "{mapper}.{caller}.{index_library.name}"
        for caller in self.config["tools"]:
            if self.config[caller].get("pedigree_wise", True):
                yield from self._yield_result_files(
                    os.path.join("output", name_pattern, "out", name_pattern + "{ext}"),
                    mapper=self.w_config["step_config"]["ngs_mapping"]["tools"]["dna"],
                    caller=[caller],
                    ext=EXT_VALUES,
                )
                yield from self._yield_result_files(
                    os.path.join("output", name_pattern, "log", name_pattern + ".{ext}"),
                    mapper=self.w_config["step_config"]["ngs_mapping"]["tools"]["dna"],
                    caller=[caller],
                    ext=(
                        "log",
                        "log.md5",
                        "conda_info.txt",
                        "conda_info.txt.md5",
                        "conda_list.txt",
                        "conda_list.txt.md5",
                    ),
                )
        # Yield result files of whole-cohort genotyping
        for caller in COHORT_WIDE_CALLERS:
            if caller in self.config["tools"] and self.config[caller]["cohort_wide"]:
                name_pattern = "{mapper}.%s.whole_cohort" % caller
                yield from expand(
                    os.path.join("output", name_pattern, "out", name_pattern + "{ext}"),
                    mapper=self.w_config["step_config"]["ngs_mapping"]["tools"]["dna"],
                    caller=[caller],
                    ext=EXT_VALUES,
                )
        # Yield report files
        yield from self._yield_bcftools_report_files()
        yield from self._yield_jannovar_report_files()

    def _yield_result_files(self, tpl, **kwargs):
        """Build output paths from path template and extension list"""
        for sheet in filter(is_not_background, self.shortcut_sheets):
            for pedigree in sheet.cohort.pedigrees:
                if not pedigree.index:
                    msg = "INFO: pedigree without index (names: {})"  # pragma: no cover
                    print(
                        msg.format(  # pragma: no cover
                            list(sorted(d.name for d in pedigree.donors))
                        ),
                        file=sys.stderr,
                    )
                    continue  # pragma: no cover
                elif not pedigree.index.dna_ngs_library:  # pragma: no cover
                    msg = "INFO: pedigree index without DNA NGS library (names: {})"
                    print(
                        msg.format(  # pragma: no cover
                            list(sorted(d.name for d in pedigree.donors))
                        ),
                        file=sys.stderr,
                    )
                    continue  # pragma: no cover
                yield from expand(tpl, index_library=[pedigree.index.dna_ngs_library], **kwargs)

    def _yield_bcftools_report_files(self):
        name_pattern = "{mapper}.{caller}.{index_library.name}"
        tpl = (
            "output/"
            + name_pattern
            + "/report/bcftools_stats/"
            + name_pattern
            + ".{donor_library.name}.{ext}"
        )
        for sheet in filter(is_not_background, self.shortcut_sheets):
            for caller in self.config["tools"]:
                if self.config[caller].get("pedigree_wise", True):
                    for pedigree in sheet.cohort.pedigrees:
                        if not pedigree.index:
                            msg = "INFO: pedigree without index (names: {})"  # pragma: no cover
                            print(
                                msg.format(  # pragma: no cover
                                    list(sorted(d.name for d in pedigree.donors))
                                ),
                                file=sys.stderr,
                            )
                            continue  # pragma: no cover
                        elif not pedigree.index.dna_ngs_library:  # pragma: no cover
                            msg = "INFO: pedigree index DNA NGS library (names: {})"
                            print(
                                msg.format(  # pragma: no cover
                                    list(sorted(d.name for d in pedigree.donors))
                                ),
                                file=sys.stderr,
                            )
                            continue  # pragma: no cover
                        for donor in pedigree.donors:
                            if donor.dna_ngs_library:
                                yield from expand(
                                    tpl,
                                    mapper=self.w_config["step_config"]["ngs_mapping"]["tools"][
                                        "dna"
                                    ],
                                    caller=[caller],
                                    index_library=[pedigree.index.dna_ngs_library],
                                    donor_library=[donor.dna_ngs_library],
                                    ext=["txt", "txt.md5"],
                                )

    def _yield_jannovar_report_files(self):
        name_pattern = "{mapper}.{caller}.{index_library.name}"
        tpl = "output/" + name_pattern + "/report/jannovar_statistics/" + name_pattern + ".{ext}"
        for sheet in filter(is_not_background, self.shortcut_sheets):
            for caller in self.config["tools"]:
                if self.config[caller].get("pedigree_wise", True):
                    for pedigree in sheet.cohort.pedigrees:
                        if not pedigree.index:
                            msg = "INFO: pedigree without index (names: {})"  # pragma: no cover
                            print(
                                msg.format(  # pragma: no cover
                                    list(sorted(d.name for d in pedigree.donors))
                                ),
                                file=sys.stderr,
                            )
                            continue  # pragma: no cover
                        elif not pedigree.index.dna_ngs_library:  # pragma: no cover
                            msg = "INFO: pedigree index without DNA NGS library (names: {})"
                            print(
                                msg.format(  # pragma: no cover
                                    list(sorted(d.name for d in pedigree.donors))
                                ),
                                file=sys.stderr,
                            )
                            continue  # pragma: no cover
                        yield from expand(
                            tpl,
                            mapper=self.w_config["step_config"]["ngs_mapping"]["tools"]["dna"],
                            caller=[caller],
                            index_library=[pedigree.index.dna_ngs_library],
                            ext=["txt", "txt.md5"],
                        )
        # Statistics for whole-cohort files
        for caller in COHORT_WIDE_CALLERS:
            if caller in self.config["tools"] and self.config[caller]["cohort_wide"]:
                name_pattern = "{mapper}.%s.whole_cohort" % caller
                tpl = (
                    "output/"
                    + name_pattern
                    + "/report/jannovar_statistics/"
                    + name_pattern
                    + ".{ext}"
                )
                yield from expand(
                    tpl,
                    mapper=self.w_config["step_config"]["ngs_mapping"]["tools"]["dna"],
                    ext=["txt", "txt.md5"],
                )

    def check_config(self):
        """Check that the path to the NGS mapping is present"""
        self.ensure_w_config(
            ("step_config", "variant_calling", "path_ngs_mapping"),
            "Path to NGS mapping not configured but required for variant calling",
        )
        self.ensure_w_config(
            ("static_data_config", "reference", "path"),
            "Path to reference FASTA not configured but required for variant calling",
        )
        # Check that only valid tools are selected
        selected = set(self.w_config["step_config"]["variant_calling"]["tools"])
        invalid = selected - set(VARIANT_CALLERS)
        if invalid:
            raise Exception(
                "Invalid variant callers selected: {}".format(  # pragma: no cover
                    list(sorted(invalid))
                )
            )
