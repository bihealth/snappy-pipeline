# PREAMBLE


shell.executable("/bin/bash")
shell.prefix("set -ex;")

configfile: 'config.json'

localrules: all


def multiply_time(day_time_str, factor):
    # Check if time contains day, ex: '1-00:00:00'
    if "-" in day_time_str:
        arr_ = day_time_str.split("-")
        days = int(arr_[0])
        time_str = arr_[1]
    else:
        days = 0
        time_str = day_time_str

    # Process based on time structure
    arr_ = time_str.split(":")
    if time_str.count(":") == 2: # hours:minutes:seconds
        seconds = int(arr_[0]) * 60 * 60 + int(arr_[1]) * 60 + int(arr_[2])
    elif time_str.count(":") == 1: # minutes:seconds
        seconds = int(arr_[0]) * 60 + int(arr_[1])
    elif time_str.count(":") == 0: # minutes
        seconds = int(time_str) * 60
    else:
        raise ValueError(f"Invalid time: {day_time_str}")
    # Add days to second
    seconds += days * 86400

    # Apply factor
    seconds = int(seconds * factor)

    # Normalise time
    (norm_days, remainder) = divmod(seconds, 86400)
    (hours, remainder) = divmod(remainder, 3600)
    (minutes, seconds) = divmod(remainder, 60)

    # Fill string - example hour '7' -> '07'
    h_str = str(hours).zfill(2)
    m_str = str(minutes).zfill(2)
    s_str = str(seconds).zfill(2)

    return "%d-%s:%s:%s" % (norm_days, h_str, m_str, s_str)


def multiply_memory(memory_str, factor):
    memory_mb = None
    suffixes = (
        ("k", 1e-3),
        ("M", 1),
        ("G", 1e3),
        ("T", 1e6),
    )
    for (suffix, mult) in suffixes:
        if memory_str.endswith(suffix):
            memory_mb = float(memory_str[:-1]) * mult
            break
    # No match, assume no suffix int
    if not memory_mb:
        memory_mb = float(memory_str)
    return int(memory_mb * factor)


def resource_chunk_threads(wildcards):
    '''Return the number of threads to use for running one chunk.'''
    return 1

def resource_chunk_partition(wildcards):
    '''Return the partition to use for running one chunk.'''
    return 'medium'

def resource_merge_threads(wildcards):
    '''Return the number of threads to use for running merging.'''
    return 1

def resource_merge_memory(wildcards):
    '''Return the memory to use for running merging.'''
    return '8G'

def resource_merge_time(wildcards):
    '''Return the time to use for running merging.'''
    return '20:00:00'

def resource_merge_partition(wildcards):
    '''Return the partition to use for running merging.'''
    return 'medium'


def resource_chunk_memory(wildcards, attempt):
    '''Return the memory to use for running one chunk.'''
    return multiply_memory('28G', attempt)

def resource_chunk_time(wildcards, attempt):
    '''Return the time to use for running one chunk.'''
    return multiply_time('12:00:00', attempt)


rule all:
    input: **{'vcf': '/work/bwa.mutect2/out/bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz', 'vcf_md5': '/work/bwa.mutect2/out/bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.md5', 'vcf_tbi': '/work/bwa.mutect2/out/bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi', 'vcf_tbi_md5': '/work/bwa.mutect2/out/bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi.md5'}


# PARALLEL WORK

rule chunk_0:
    input:
        **{'normal_bai': 'NGS_MAPPING/output/bwa.P001-N1-DNA1-WGS1/out/bwa.P001-N1-DNA1-WGS1.bam.bai', 'normal_bam': 'NGS_MAPPING/output/bwa.P001-N1-DNA1-WGS1/out/bwa.P001-N1-DNA1-WGS1.bam'}
    output:
        **{'vcf': 'job_out.0.d/out/tmp_0.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz', 'vcf_md5': 'job_out.0.d/out/tmp_0.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.md5', 'vcf_tbi': 'job_out.0.d/out/tmp_0.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi', 'vcf_tbi_md5': 'job_out.0.d/out/tmp_0.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi.md5'}
    log:
        **{'conda_info': 'job_out.0.d/log/tmp_0.bwa.mutect2.P001-N1-DNA1-WGS1.conda_info.txt', 'conda_info_md5': 'job_out.0.d/log/tmp_0.bwa.mutect2.P001-N1-DNA1-WGS1.conda_info.txt.md5', 'conda_list': 'job_out.0.d/log/tmp_0.bwa.mutect2.P001-N1-DNA1-WGS1.conda_list.txt', 'conda_list_md5': 'job_out.0.d/log/tmp_0.bwa.mutect2.P001-N1-DNA1-WGS1.conda_list.txt.md5', 'log': 'job_out.0.d/log/tmp_0.bwa.mutect2.P001-N1-DNA1-WGS1.log', 'log_md5': 'job_out.0.d/log/tmp_0.bwa.mutect2.P001-N1-DNA1-WGS1.log.md5'}
    threads: resource_chunk_threads
    resources:
        time=resource_chunk_time,
        memory=resource_chunk_memory,
        partition=resource_chunk_partition,
    params:
        **{'args': {'intervals': ['1:1-50,000,000']}}


rule chunk_1:
    input:
        **{'normal_bai': 'NGS_MAPPING/output/bwa.P001-N1-DNA1-WGS1/out/bwa.P001-N1-DNA1-WGS1.bam.bai', 'normal_bam': 'NGS_MAPPING/output/bwa.P001-N1-DNA1-WGS1/out/bwa.P001-N1-DNA1-WGS1.bam'}
    output:
        **{'vcf': 'job_out.1.d/out/tmp_1.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz', 'vcf_md5': 'job_out.1.d/out/tmp_1.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.md5', 'vcf_tbi': 'job_out.1.d/out/tmp_1.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi', 'vcf_tbi_md5': 'job_out.1.d/out/tmp_1.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi.md5'}
    log:
        **{'conda_info': 'job_out.1.d/log/tmp_1.bwa.mutect2.P001-N1-DNA1-WGS1.conda_info.txt', 'conda_info_md5': 'job_out.1.d/log/tmp_1.bwa.mutect2.P001-N1-DNA1-WGS1.conda_info.txt.md5', 'conda_list': 'job_out.1.d/log/tmp_1.bwa.mutect2.P001-N1-DNA1-WGS1.conda_list.txt', 'conda_list_md5': 'job_out.1.d/log/tmp_1.bwa.mutect2.P001-N1-DNA1-WGS1.conda_list.txt.md5', 'log': 'job_out.1.d/log/tmp_1.bwa.mutect2.P001-N1-DNA1-WGS1.log', 'log_md5': 'job_out.1.d/log/tmp_1.bwa.mutect2.P001-N1-DNA1-WGS1.log.md5'}
    threads: resource_chunk_threads
    resources:
        time=resource_chunk_time,
        memory=resource_chunk_memory,
        partition=resource_chunk_partition,
    params:
        **{'args': {'intervals': ['1:50,000,001-100,000,000']}}


rule chunk_2:
    input:
        **{'normal_bai': 'NGS_MAPPING/output/bwa.P001-N1-DNA1-WGS1/out/bwa.P001-N1-DNA1-WGS1.bam.bai', 'normal_bam': 'NGS_MAPPING/output/bwa.P001-N1-DNA1-WGS1/out/bwa.P001-N1-DNA1-WGS1.bam'}
    output:
        **{'vcf': 'job_out.2.d/out/tmp_2.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz', 'vcf_md5': 'job_out.2.d/out/tmp_2.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.md5', 'vcf_tbi': 'job_out.2.d/out/tmp_2.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi', 'vcf_tbi_md5': 'job_out.2.d/out/tmp_2.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi.md5'}
    log:
        **{'conda_info': 'job_out.2.d/log/tmp_2.bwa.mutect2.P001-N1-DNA1-WGS1.conda_info.txt', 'conda_info_md5': 'job_out.2.d/log/tmp_2.bwa.mutect2.P001-N1-DNA1-WGS1.conda_info.txt.md5', 'conda_list': 'job_out.2.d/log/tmp_2.bwa.mutect2.P001-N1-DNA1-WGS1.conda_list.txt', 'conda_list_md5': 'job_out.2.d/log/tmp_2.bwa.mutect2.P001-N1-DNA1-WGS1.conda_list.txt.md5', 'log': 'job_out.2.d/log/tmp_2.bwa.mutect2.P001-N1-DNA1-WGS1.log', 'log_md5': 'job_out.2.d/log/tmp_2.bwa.mutect2.P001-N1-DNA1-WGS1.log.md5'}
    threads: resource_chunk_threads
    resources:
        time=resource_chunk_time,
        memory=resource_chunk_memory,
        partition=resource_chunk_partition,
    params:
        **{'args': {'intervals': ['1:100,000,001-150,000,000']}}


rule chunk_3:
    input:
        **{'normal_bai': 'NGS_MAPPING/output/bwa.P001-N1-DNA1-WGS1/out/bwa.P001-N1-DNA1-WGS1.bam.bai', 'normal_bam': 'NGS_MAPPING/output/bwa.P001-N1-DNA1-WGS1/out/bwa.P001-N1-DNA1-WGS1.bam'}
    output:
        **{'vcf': 'job_out.3.d/out/tmp_3.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz', 'vcf_md5': 'job_out.3.d/out/tmp_3.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.md5', 'vcf_tbi': 'job_out.3.d/out/tmp_3.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi', 'vcf_tbi_md5': 'job_out.3.d/out/tmp_3.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi.md5'}
    log:
        **{'conda_info': 'job_out.3.d/log/tmp_3.bwa.mutect2.P001-N1-DNA1-WGS1.conda_info.txt', 'conda_info_md5': 'job_out.3.d/log/tmp_3.bwa.mutect2.P001-N1-DNA1-WGS1.conda_info.txt.md5', 'conda_list': 'job_out.3.d/log/tmp_3.bwa.mutect2.P001-N1-DNA1-WGS1.conda_list.txt', 'conda_list_md5': 'job_out.3.d/log/tmp_3.bwa.mutect2.P001-N1-DNA1-WGS1.conda_list.txt.md5', 'log': 'job_out.3.d/log/tmp_3.bwa.mutect2.P001-N1-DNA1-WGS1.log', 'log_md5': 'job_out.3.d/log/tmp_3.bwa.mutect2.P001-N1-DNA1-WGS1.log.md5'}
    threads: resource_chunk_threads
    resources:
        time=resource_chunk_time,
        memory=resource_chunk_memory,
        partition=resource_chunk_partition,
    params:
        **{'args': {'intervals': ['1:150,000,001-200,000,000']}}


rule chunk_4:
    input:
        **{'normal_bai': 'NGS_MAPPING/output/bwa.P001-N1-DNA1-WGS1/out/bwa.P001-N1-DNA1-WGS1.bam.bai', 'normal_bam': 'NGS_MAPPING/output/bwa.P001-N1-DNA1-WGS1/out/bwa.P001-N1-DNA1-WGS1.bam'}
    output:
        **{'vcf': 'job_out.4.d/out/tmp_4.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz', 'vcf_md5': 'job_out.4.d/out/tmp_4.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.md5', 'vcf_tbi': 'job_out.4.d/out/tmp_4.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi', 'vcf_tbi_md5': 'job_out.4.d/out/tmp_4.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi.md5'}
    log:
        **{'conda_info': 'job_out.4.d/log/tmp_4.bwa.mutect2.P001-N1-DNA1-WGS1.conda_info.txt', 'conda_info_md5': 'job_out.4.d/log/tmp_4.bwa.mutect2.P001-N1-DNA1-WGS1.conda_info.txt.md5', 'conda_list': 'job_out.4.d/log/tmp_4.bwa.mutect2.P001-N1-DNA1-WGS1.conda_list.txt', 'conda_list_md5': 'job_out.4.d/log/tmp_4.bwa.mutect2.P001-N1-DNA1-WGS1.conda_list.txt.md5', 'log': 'job_out.4.d/log/tmp_4.bwa.mutect2.P001-N1-DNA1-WGS1.log', 'log_md5': 'job_out.4.d/log/tmp_4.bwa.mutect2.P001-N1-DNA1-WGS1.log.md5'}
    threads: resource_chunk_threads
    resources:
        time=resource_chunk_time,
        memory=resource_chunk_memory,
        partition=resource_chunk_partition,
    params:
        **{'args': {'intervals': ['1:200,000,001-249,250,621']}}


# JOIN PARALLEL RESULTS

rule merge_chunk_0:
    input: **{'vcf': ['job_out.0.d/out/tmp_0.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz', 'job_out.1.d/out/tmp_1.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz', 'job_out.2.d/out/tmp_2.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz'], 'vcf_md5': ['job_out.0.d/out/tmp_0.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.md5', 'job_out.1.d/out/tmp_1.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.md5', 'job_out.2.d/out/tmp_2.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.md5'], 'vcf_tbi': ['job_out.0.d/out/tmp_0.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi', 'job_out.1.d/out/tmp_1.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi', 'job_out.2.d/out/tmp_2.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi'], 'vcf_tbi_md5': ['job_out.0.d/out/tmp_0.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi.md5', 'job_out.1.d/out/tmp_1.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi.md5', 'job_out.2.d/out/tmp_2.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi.md5']}
    output: **{'vcf': 'merge_out.0.d/out/merge_0.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz', 'vcf_md5': 'merge_out.0.d/out/merge_0.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.md5', 'vcf_tbi': 'merge_out.0.d/out/merge_0.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi', 'vcf_tbi_md5': 'merge_out.0.d/out/merge_0.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi.md5'}
    threads: resource_merge_threads
    resources:
        time=resource_merge_time,
        memory=resource_merge_memory,
        partition=resource_merge_partition,
    shell:
        r'''
        set -euo pipefail  # Unofficial Bash strict mode

        # Merge chunks output ------------------------------------------
        
# Concatenate raw calls vcfs & index result ----------------------
bcftools concat \
    --allow-overlaps \
    -d none \
    -o {output.vcf} \
    -O z \
    {input.vcf}
tabix -f {output.vcf}

# Compute md5 sums -----------------------------------------------
pushd $(dirname {output.vcf})
f=$(basename {output.vcf})
md5sum $f > $f.md5
md5sum $f.tbi > $f.tbi.md5
popd


        # Save chunk logs in tarball -----------------------------------
        mkdir -p $(dirname merge_out.0.d/log/merge.tar.gz)
        tar -zcvf merge_out.0.d/log/merge.tar.gz job_out.0.d/log/* job_out.1.d/log/* job_out.2.d/log/*
        pushd $(dirname merge_out.0.d/log/merge.tar.gz)
        f=$(basename merge_out.0.d/log/merge.tar.gz)
        md5sum $f > $f.md5
        popd
        '''


rule merge_chunk_1:
    input: **{'vcf': ['job_out.3.d/out/tmp_3.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz', 'job_out.4.d/out/tmp_4.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz'], 'vcf_md5': ['job_out.3.d/out/tmp_3.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.md5', 'job_out.4.d/out/tmp_4.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.md5'], 'vcf_tbi': ['job_out.3.d/out/tmp_3.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi', 'job_out.4.d/out/tmp_4.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi'], 'vcf_tbi_md5': ['job_out.3.d/out/tmp_3.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi.md5', 'job_out.4.d/out/tmp_4.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi.md5']}
    output: **{'vcf': 'merge_out.1.d/out/merge_1.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz', 'vcf_md5': 'merge_out.1.d/out/merge_1.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.md5', 'vcf_tbi': 'merge_out.1.d/out/merge_1.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi', 'vcf_tbi_md5': 'merge_out.1.d/out/merge_1.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi.md5'}
    threads: resource_merge_threads
    resources:
        time=resource_merge_time,
        memory=resource_merge_memory,
        partition=resource_merge_partition,
    shell:
        r'''
        set -euo pipefail  # Unofficial Bash strict mode

        # Merge chunks output ------------------------------------------
        
# Concatenate raw calls vcfs & index result ----------------------
bcftools concat \
    --allow-overlaps \
    -d none \
    -o {output.vcf} \
    -O z \
    {input.vcf}
tabix -f {output.vcf}

# Compute md5 sums -----------------------------------------------
pushd $(dirname {output.vcf})
f=$(basename {output.vcf})
md5sum $f > $f.md5
md5sum $f.tbi > $f.tbi.md5
popd


        # Save chunk logs in tarball -----------------------------------
        mkdir -p $(dirname merge_out.1.d/log/merge.tar.gz)
        tar -zcvf merge_out.1.d/log/merge.tar.gz job_out.3.d/log/* job_out.4.d/log/*
        pushd $(dirname merge_out.1.d/log/merge.tar.gz)
        f=$(basename merge_out.1.d/log/merge.tar.gz)
        md5sum $f > $f.md5
        popd
        '''


rule merge_all:
    input: **{'vcf': ['merge_out.0.d/out/merge_0.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz', 'merge_out.1.d/out/merge_1.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz'], 'vcf_md5': ['merge_out.0.d/out/merge_0.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.md5', 'merge_out.1.d/out/merge_1.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.md5'], 'vcf_tbi': ['merge_out.0.d/out/merge_0.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi', 'merge_out.1.d/out/merge_1.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi'], 'vcf_tbi_md5': ['merge_out.0.d/out/merge_0.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi.md5', 'merge_out.1.d/out/merge_1.bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi.md5']}
    output: **{'vcf': '/work/bwa.mutect2/out/bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz', 'vcf_md5': '/work/bwa.mutect2/out/bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.md5', 'vcf_tbi': '/work/bwa.mutect2/out/bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi', 'vcf_tbi_md5': '/work/bwa.mutect2/out/bwa.mutect2.P001-N1-DNA1-WGS1.vcf.gz.tbi.md5'}
    log: **{'conda_info': '/work/bwa.mutect2/log/bwa.mutect2.P001-N1-DNA1-WGS1.conda_info.txt', 'conda_info_md5': '/work/bwa.mutect2/log/bwa.mutect2.P001-N1-DNA1-WGS1.conda_info.txt.md5', 'conda_list': '/work/bwa.mutect2/log/bwa.mutect2.P001-N1-DNA1-WGS1.conda_list.txt', 'conda_list_md5': '/work/bwa.mutect2/log/bwa.mutect2.P001-N1-DNA1-WGS1.conda_list.txt.md5', 'log': '/work/bwa.mutect2/log/bwa.mutect2.P001-N1-DNA1-WGS1.log', 'log_md5': '/work/bwa.mutect2/log/bwa.mutect2.P001-N1-DNA1-WGS1.log.md5'}
    threads: resource_merge_threads
    resources:
        time=resource_merge_time,
        memory=resource_merge_memory,
        partition=resource_merge_partition,
    shell:
        r'''
        set -euo pipefail  # Unofficial Bash strict mode

        # Merge chunks output ------------------------------------------
        
# Concatenate raw calls vcfs & index result ----------------------
bcftools concat \
    --allow-overlaps \
    -d none \
    -o {output.vcf} \
    -O z \
    {input.vcf}
tabix -f {output.vcf}

# Compute md5 sums -----------------------------------------------
pushd $(dirname {output.vcf})
f=$(basename {output.vcf})
md5sum $f > $f.md5
md5sum $f.tbi > $f.tbi.md5
popd


        # Save chunk logs in tarball -----------------------------------
        mkdir -p $(dirname /work/bwa.mutect2/log/bwa.mutect2.P001-N1-DNA1-WGS1.log.merge.tar.gz)
        tar -zcvf /work/bwa.mutect2/log/bwa.mutect2.P001-N1-DNA1-WGS1.log.merge.tar.gz merge_out.0.d/log/* merge_out.1.d/log/*
        pushd $(dirname /work/bwa.mutect2/log/bwa.mutect2.P001-N1-DNA1-WGS1.log.merge.tar.gz)
        f=$(basename /work/bwa.mutect2/log/bwa.mutect2.P001-N1-DNA1-WGS1.log.merge.tar.gz)
        md5sum $f > $f.md5
        popd
        '''


# EPILOGUE

